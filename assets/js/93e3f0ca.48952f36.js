"use strict";(self.webpackChunkbacalhau_docs=self.webpackChunkbacalhau_docs||[]).push([[5846],{3905:(e,t,o)=>{o.d(t,{Zo:()=>c,kt:()=>p});var a=o(7294);function r(e,t,o){return t in e?Object.defineProperty(e,t,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[t]=o,e}function n(e,t){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),o.push.apply(o,a)}return o}function i(e){for(var t=1;t<arguments.length;t++){var o=null!=arguments[t]?arguments[t]:{};t%2?n(Object(o),!0).forEach((function(t){r(e,t,o[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):n(Object(o)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(o,t))}))}return e}function s(e,t){if(null==e)return{};var o,a,r=function(e,t){if(null==e)return{};var o,a,r={},n=Object.keys(e);for(a=0;a<n.length;a++)o=n[a],t.indexOf(o)>=0||(r[o]=e[o]);return r}(e,t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(a=0;a<n.length;a++)o=n[a],t.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(r[o]=e[o])}return r}var l=a.createContext({}),u=function(e){var t=a.useContext(l),o=t;return e&&(o="function"==typeof e?e(t):i(i({},t),e)),o},c=function(e){var t=u(e.components);return a.createElement(l.Provider,{value:t},e.children)},h={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var o=e.components,r=e.mdxType,n=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),d=u(o),p=r,b=d["".concat(l,".").concat(p)]||d[p]||h[p]||n;return o?a.createElement(b,i(i({ref:t},c),{},{components:o})):a.createElement(b,i({ref:t},c))}));function p(e,t){var o=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var n=o.length,i=new Array(n);i[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var u=2;u<n;u++)i[u]=o[u];return a.createElement.apply(null,i)}return a.createElement.apply(null,o)}d.displayName="MDXCreateElement"},4827:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>n,metadata:()=>s,toc:()=>u});var a=o(3117),r=(o(7294),o(3905));const n={sidebar_label:"Architecture",sidebar_position:2},i="Architecture",s={unversionedId:"about-bacalhau/architecture",id:"about-bacalhau/architecture",title:"Architecture",description:"Purpose",source:"@site/docs/about-bacalhau/architecture.md",sourceDirName:"about-bacalhau",slug:"/about-bacalhau/architecture",permalink:"/about-bacalhau/architecture",draft:!1,editUrl:"https://github.com/bacalhau-project/docs.bacalhau.org/blob/main/docs/about-bacalhau/architecture.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_label:"Architecture",sidebar_position:2},sidebar:"documentationSidebar",previous:{title:"Introduction",permalink:"/about-bacalhau/introduction"},next:{title:"Landscape",permalink:"/about-bacalhau/compute-landscape"}},l={},u=[{value:"Purpose",id:"purpose",level:2},{value:"System Components",id:"system-components",level:2},{value:"Transport (interface)",id:"transport-interface",level:3},{value:"Requester node (component)",id:"requester-node-component",level:3},{value:"Compute node (component)",id:"compute-node-component",level:3},{value:"Executor (interface)",id:"executor-interface",level:3},{value:"Storage Provider (interface)",id:"storage-provider-interface",level:3},{value:"Verifier (interface)",id:"verifier-interface",level:3},{value:"Publisher (interface)",id:"publisher-interface",level:3},{value:"Job Lifecycle",id:"job-lifecycle",level:2},{value:"Job Submission",id:"job-submission",level:3},{value:"Job Acceptance",id:"job-acceptance",level:3},{value:"Job Execution",id:"job-execution",level:3},{value:"Verification",id:"verification",level:3},{value:"Publishing",id:"publishing",level:3},{value:"Networking",id:"networking",level:3},{value:"Input / Output Volumes",id:"input--output-volumes",level:3}],c={toc:u};function h(e){let{components:t,...n}=e;return(0,r.kt)("wrapper",(0,a.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"architecture"},"Architecture"),(0,r.kt)("h2",{id:"purpose"},"Purpose"),(0,r.kt)("p",null,'The purpose of Bacalhau is to provide a platform for public, transparent, and optionally verifiable computation. Bacalhau enables users to run arbitrary docker containers and wasm images as tasks against data stored in IPFS. This architecture is also referred to as Compute Over Data (or CoD). The Portuguese word for salted Cod fish is "Bacalhau" which is the origin of the project\'s name.'),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"image",src:o(7762).Z,width:"1152",height:"864"})),(0,r.kt)("p",null,"Bacalhau operates as a peer-to-peer network of nodes where each node has both a requestor and compute component.  To interact with the cluster - Bacalhau CLI requests are sent to a node in the cluster (via JSON over HTTP), which then broadcasts messages over the transport layer to other nodes in the cluster.  All other nodes in the network are connected to the transport layer and as such have a shared view of the world."),(0,r.kt)("h2",{id:"system-components"},"System Components"),(0,r.kt)("p",null,"Bacalhau's architecture is divided among the following core components and interfaces: "),(0,r.kt)("h3",{id:"transport-interface"},"Transport (interface)"),(0,r.kt)("p",null,"The transport component is responsible for connecting different bacalhau nodes in the peer to peer network. Its job is to broadcast messages about jobs as they are created, bid upon and executed by compute nodes."),(0,r.kt)("p",null,"As well as handling the distribution of messages to other nodes, It\u2019s also responsible for handling the \u201cidentity\u201d of an individual bacalhau node."),(0,r.kt)("p",null,"The main implementation of the transport interface in a production Bacalhau network is the ",(0,r.kt)("a",{parentName:"p",href:"https://libp2p.io/"},"libp2p")," transport.  This uses the ",(0,r.kt)("a",{parentName:"p",href:"https://docs.libp2p.io/concepts/publish-subscribe/"},"GossipSub")," handler to distribute job messages to other nodes on the network."),(0,r.kt)("h3",{id:"requester-node-component"},"Requester node (component)"),(0,r.kt)("p",null,"The requestor node is responsible for handling requests from clients using JSON over HTTP and is the main \u201ccustodian\u201d of jobs submitted to it."),(0,r.kt)("p",null,"When you submit a job to a given Requestor node - it handles the process of broadcasting that job to the network and then accepting or rejecting the various bids that will come back in for that job.  There is only ever a single requestor node for a given job and that is the requestor node that job was originally submitted to."),(0,r.kt)("p",null,"Once compute nodes have executed the job - they will produce \u201cverification proposals\u201d which the requester node will collate and combine when enough have been proposed.  At this point - the proposals will be \u201caccepted\u201d or \u201crejected\u201d and the compute nodes will then publish their raw results."),(0,r.kt)("h3",{id:"compute-node-component"},"Compute node (component)"),(0,r.kt)("p",null,"When a new job is seen on the network - the Compute node will decide whether it wants to \u201cbid\u201d on that job or not.  If a bid is made and subsequently accepted by the requester node - a \u201cbid accepted\u201d event will then trigger the Compute node to run the job using its collection of \u201cexecutors\u201d (each of which in turn has a collection of \u201cstorage providers\u201d)."),(0,r.kt)("p",null,"Once the executor has run the job and has produced some results - the Compute node will then produce a verification proposal which the requester node will collate alongside proposals from other compute nodes that ran the same job.  These proposals will then be accepted or rejected which will result in the compute node then publishing the raw results (via the publisher interface).  The Compute node has a collection of named executors, verifiers and publishers and will pick the most appropriate ones based on the job spec."),(0,r.kt)("h3",{id:"executor-interface"},"Executor (interface)"),(0,r.kt)("p",null,"The Executor is what actually \u201cruns\u201d the job and checks for the locality of storage used by a job.   It will handle \u201cpresenting\u201d the input and output storage volumes into the job when it is run."),(0,r.kt)("p",null,"Storage means something entirely different between docker and WASM and so if a job mentions \u201cuse this IPFS cid\u201d - it will result in two different storage providers being used depending on if the job is using the docker or WASM executor."),(0,r.kt)("p",null,"Put another way - the executor has two main tasks:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Present the storage volumes in a way that is appropriate for the executor."),(0,r.kt)("li",{parentName:"ul"},"Run the job.")),(0,r.kt)("p",null,"When it\u2019s finished running the job - the executor will combine stdout, stderr and named output volumes into a \u201cresults folder\u201d.  This results folder is then used to create the verification proposal which is sent off to the requester.  Once \u201cresults accepted\u201d or \u201cresults rejected\u201d events are seen - this results folder is then sent off to the publisher to be published."),(0,r.kt)("h3",{id:"storage-provider-interface"},"Storage Provider (interface)"),(0,r.kt)("p",null,"Storage providers are responsible for presenting some upstream storage source (e.g. ipfs cid) into an executor in an opinionated way."),(0,r.kt)("p",null,"For example - we might have the following two storage providers:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"IPFS posix - a storage provider that manifests a CID as a POSIX filesystem"),(0,r.kt)("li",{parentName:"ul"},"IPFS library - a storage provider that streams the contents of a CID via a library call")),(0,r.kt)("p",null,"And we might have the following two executor implementations:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Docker - run docker containers"),(0,r.kt)("li",{parentName:"ul"},"WASM - run WASM binaries")),(0,r.kt)("p",null,"If we submit a job with a volume of type \u201cipfs\u201d to both executors - it should result in the docker executor using the \u201cIPFS posix\u201d storage provider and the WASM executor using the \u201cIPFS library\u201d provider."),(0,r.kt)("p",null,"As such - an executor implementation will contain the storage providers it can work with and they are loosely coupled e.g. the IPFS posix & library storage providers can be used across multiple executors where appropriate."),(0,r.kt)("h3",{id:"verifier-interface"},"Verifier (interface)"),(0,r.kt)("p",null,"The verifier takes over once the executor has run the job.  Its main two tasks are to check the results produced by the executor (against results produced by other nodes) and to transport those results back to the requester node."),(0,r.kt)("p",null,"How the results are checked depend on the nature of the job.  For example - if the job is deterministic - the \u201cdeterministic hash\u201d verifier can be used but if the job is non-deterministic, another approach must be used."),(0,r.kt)("p",null,"The verifier exists on both the compute node and requester node - the task of the compute node verifier is to produce a \u201cverification proposal\u201d based on having run a job.  The task of the requester node verifier is to collate the proposals from the various compute nodes and when enough proposals have arrived - to decide on which compute nodes have actually performed the work.  Once it has decided - it will emit events to the network \u201caccepting\u201d or \u201crejecting\u201d the verification proposals."),(0,r.kt)("h3",{id:"publisher-interface"},"Publisher (interface)"),(0,r.kt)("p",null,"Once verification has been complete - the publisher will handle uploading the raw results to a place that clients can read from.  It\u2019s important to make a distinction between verification proposals and published results.  Before verification has happened - the published results need to remain private between the compute node that ran the job and the requester node looking after that job.  This is to prevent compute nodes simply copying the results produced by other nodes."),(0,r.kt)("p",null,"The publisher interface is responsible for uploading the local folder of results to somewhere that can be read by the rest of the world.  The default publisher is either Estuary (if an API key has been provided) or IPFS.  In both cases - the published results of a job will end up on IPFS with a cid that can be used to read them.  If Estuary is used as the publisher then the results will also end up on Filecoin."),(0,r.kt)("h2",{id:"job-lifecycle"},"Job Lifecycle"),(0,r.kt)("h3",{id:"job-submission"},"Job Submission"),(0,r.kt)("p",null,"Jobs submitted via the Bacalhau CLI are forwarded to a bacalhau cluster node at bootstrap.production.bacalhau.org via port 1234 by default. This bacalhau node will act as the \u201crequestor node\u201d for the duration of the job lifecycle."),(0,r.kt)("p",null,"When jobs are submitted to the requestor node - all compute nodes hear of this new job and can choose to \u201cbid\u201d on it.  The job deal will have a \u201cconcurrency\u201d setting which means \u201chow many different nodes I want to run this job\u201d.  It will also have \u201cconfidence\u201d and \u201cmin-bids\u201d properties.  Confidence is how many verification proposals must agree for the job to be deemed successful.  Min bids is how many bids must have been made before we will choose to accept any."),(0,r.kt)("p",null,"The job might also mention the use of \u201cvolumes\u201d (for example some IPFS CIDs).  The compute node can choose to bid on the job if the data for the volumes resides locally to the compute node or it can choose to bid anyway.  Bacalhau supports the use of external http or exec hooks to decide if a node wants to bid on a job.  This means a node operator can give fine grained rules as to what jobs they are willing to run."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"image",src:o(6808).Z,width:"1152",height:"864"})),(0,r.kt)("h3",{id:"job-acceptance"},"Job Acceptance"),(0,r.kt)("p",null,"As these bids from compute nodes arrive back at the originating requester node - it can choose which bids to accept and which ones to reject.  This can be based on the previous reputation of each compute node or any other factors the requestor node might take into account (like locality, hardware resources, cost etc).  The requestor node will also have the same http or exec hooks to decide if it wants to accept a bid from a given compute node.  This means a node operator can give fine grained rules as to what bids they are willing to accept.  The \u201cmin-bids\u201d setting is useful to ensure that we don\u2019t accept bids on a first bid first accepted basis."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"image",src:o(476).Z,width:"1152",height:"864"})),(0,r.kt)("h3",{id:"job-execution"},"Job Execution"),(0,r.kt)("p",null,"As accepted bids are received by compute nodes - they will \u201cexecute\u201d the job using the executor for that job and the storage providers that executor has mapped in."),(0,r.kt)("p",null,"For example - a job could use the \u201cdocker\u201d executor and \u201cipfs\u201d storage volumes.  This would result in a POSIX mount of the IPFS storage into a running container.  Alternatively - a job could use the \u201cWASM\u201d executor and \u201cipfs\u201d storage volumes.  This would result in a WASM style syscall to stream the storage bytes into the WASM runtime.  The point is that each \u201cexecutor\u201d will deal with storage in a different way and so even though each job mentions \u201cipfs\u201d storage volumes - they would both end up with different implementations at runtime."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"image",src:o(4608).Z,width:"1152",height:"864"})),(0,r.kt)("h3",{id:"verification"},"Verification"),(0,r.kt)("p",null,"Once the executor has completed the running of the job - a \u201cverification proposal\u201d will be generated by the verifier module running on the compute node.  The nature of this proposal depends on the module used - for example the \u201cdeterministic hash\u201d verifier will:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Calculate a sha256 hash of the contents of the results folder"),(0,r.kt)("li",{parentName:"ul"},"Encrypt this hash using the public key of the requester node"),(0,r.kt)("li",{parentName:"ul"},"Broadcast the encrypted hash over the network",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Nodes that are NOT the requester node cannot copy the hash because they do not have the requesters private key"),(0,r.kt)("li",{parentName:"ul"},"The requester will use it\u2019s private key to decrypt the message and read the hash"),(0,r.kt)("li",{parentName:"ul"},"This means that bad actors cannot simply copy the results hash from other nodes"))),(0,r.kt)("li",{parentName:"ul"},"The requester node will wait for enough proposals before comparing the results hashes"),(0,r.kt)("li",{parentName:"ul"},"It will then broadcast \u201cresults accepted\u201d and \u201cresults rejected\u201d events based on it\u2019s decision for verification")),(0,r.kt)("p",null,"It\u2019s possible to use other types of verification methods by re-implementing the verification interface and using another technique."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"image",src:o(5671).Z,width:"1152",height:"864"})),(0,r.kt)("h3",{id:"publishing"},"Publishing"),(0,r.kt)("p",null,"Once verification has resulted in \u201cresults accepted\u201d or \u201cresults rejected\u201d events - the publisher will publish the raw results folder currently residing on the compute node."),(0,r.kt)("p",null,"The default publisher is \u201cEstuary\u201d (if no API key is provided this falls back to the IPFS publisher).  The publisher interface is really simple - it mainly consists of a single function that has the task of uploading the local results folder somewhere and returning a storage reference to where it has been uploaded."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"image",src:o(8562).Z,width:"1152",height:"864"})),(0,r.kt)("h3",{id:"networking"},"Networking"),(0,r.kt)("p",null,"Jobs should only require dependencies that are baked into their Docker images and the input files mounted from IPFS in order to produce their output, therefore egress access to the network is currently disabled."),(0,r.kt)("h3",{id:"input--output-volumes"},"Input / Output Volumes"),(0,r.kt)("p",null,"A job includes the concept of input and output volumes and the docker executor implements support for these. This means you can specific ipfs CIDs and input paths and also write results to an output volume - this can be seen by the following example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"cid=$(ipfs add file.txt)\nbacalhau docker run \\\n  -v $cid:/file.txt \\\n  -o apples:/output_folder \\\n  ubuntu \\\n  bash -c 'cat /file.txt > /output_folder/file.txt'\n")),(0,r.kt)("p",null,"The above example demonstrates an input volume flag \u201c-v $cid:/file.txt\u201d, which mounts the contents of $cid within the docker container at location /file.txt (root)."),(0,r.kt)("p",null,"Output volumes are mounted to the docker container at the location specified. In the example above, any content written to /output_folder will be made available within the apples folder in the job results CID."),(0,r.kt)("p",null,"Once the job has run on the executor - the contents of stdout and stderr will be added to any named output volumes the job has used (in this case apples) and all those entities will be packaged into the results folder which is then published to ipfs via the verifier."))}h.isMDXComponent=!0},476:(e,t,o)=>{o.d(t,{Z:()=>a});const a=o.p+"assets/images/architecture-accept-job-bid-c541f3a23a90c2cfb07b47d401b202ca.jpeg"},6808:(e,t,o)=>{o.d(t,{Z:()=>a});const a=o.p+"assets/images/architecture-bid-on-job-7c64a8dd1e973cf99d718cb59cc2d2b0.jpeg"},4608:(e,t,o)=>{o.d(t,{Z:()=>a});const a=o.p+"assets/images/architecture-execute-job-2bac1c0560e5a8804b88b118ac04f1f2.jpeg"},8562:(e,t,o)=>{o.d(t,{Z:()=>a});const a=o.p+"assets/images/architecture-publishing-8f865f05dff72589a09fb378056cbc3a.jpeg"},7762:(e,t,o)=>{o.d(t,{Z:()=>a});const a=o.p+"assets/images/architecture-purpose-f192f229e16abe177d77f146ab2dca30.jpeg"},5671:(e,t,o)=>{o.d(t,{Z:()=>a});const a=o.p+"assets/images/architecture-verification-0ec9f7e4ade22c66791960bbae7f09f3.jpeg"}}]);