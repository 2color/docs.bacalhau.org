"use strict";(self.webpackChunkbacalhau_docs=self.webpackChunkbacalhau_docs||[]).push([[8726],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>d});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),u=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=u(e.components);return a.createElement(s.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),m=u(n),d=r,h=m["".concat(s,".").concat(d)]||m[d]||p[d]||o;return n?a.createElement(h,i(i({ref:t},c),{},{components:n})):a.createElement(h,i({ref:t},c))}));function d(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,i[1]=l;for(var u=2;u<o;u++)i[u]=n[u];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},4201:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>p,frontMatter:()=>o,metadata:()=>l,toc:()=>u});var a=n(7462),r=(n(7294),n(3905));const o={sidebar_label:"Stable-Diffusion-Keras-GPU",sidebar_position:2},i="Stable Diffusion Keras [GPU]",l={unversionedId:"examples/model-inference/gpu-keras-stable-diffusion/index",id:"examples/model-inference/gpu-keras-stable-diffusion/index",title:"Stable Diffusion Keras [GPU]",description:"Open In Colab",source:"@site/docs/examples/model-inference/gpu-keras-stable-diffusion/index.md",sourceDirName:"examples/model-inference/gpu-keras-stable-diffusion",slug:"/examples/model-inference/gpu-keras-stable-diffusion/",permalink:"/examples/model-inference/gpu-keras-stable-diffusion/",draft:!1,editUrl:"https://github.com/bacalhau-project/docs.bacalhau.org/blob/main/docs/examples/model-inference/gpu-keras-stable-diffusion/index.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_label:"Stable-Diffusion-Keras-GPU",sidebar_position:2},sidebar:"documentationSidebar",previous:{title:"Stable-Diffusion-CPU",permalink:"/examples/model-inference/stable-diffusion/"},next:{title:"YOLO-Object-Detection",permalink:"/examples/model-inference/object-detection-yolo5/"}},s={},u=[{value:"<strong>Prompt</strong>",id:"prompt",level:4},{value:"<strong>Output</strong>",id:"output",level:4},{value:"Install GPU requirements",id:"install-gpu-requirements",level:2},{value:"Let&#39;s instantiate a Text2Image generator and make a first image",id:"lets-instantiate-a-text2image-generator-and-make-a-first-image",level:2},{value:"Running the script with arguments",id:"running-the-script-with-arguments",level:3},{value:"Prompt",id:"prompt-1",level:4},{value:"Number of iterations",id:"number-of-iterations",level:4},{value:"Batch Size (No of images to generate)",id:"batch-size-no-of-images-to-generate",level:4},{value:"<strong>Building and Running on docker</strong>",id:"building-and-running-on-docker",level:2},{value:"<strong>Running the container on bacalhau</strong>",id:"running-the-container-on-bacalhau",level:2}],c={toc:u};function p(e){let{components:t,...o}=e;return(0,r.kt)("wrapper",(0,a.Z)({},c,o,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"stable-diffusion-keras-gpu"},"Stable Diffusion Keras ","[GPU]"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://colab.research.google.com/github/bacalhau-project/examples/blob/main/model-inference/gpu-keras-stable-diffusion/index.ipynb"},(0,r.kt)("img",{parentName:"a",src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})),"\n",(0,r.kt)("a",{parentName:"p",href:"https://mybinder.org/v2/gh/bacalhau-project/examples/HEAD?labpath=model-inference/gpu-keras-stable-diffusion/index.ipynb"},(0,r.kt)("img",{parentName:"a",src:"https://mybinder.org/badge.svg",alt:"Open In Binder"}))),(0,r.kt)("h4",{id:"prompt"},(0,r.kt)("strong",{parentName:"h4"},"Prompt")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Speed of Light\n")),(0,r.kt)("h4",{id:"output"},(0,r.kt)("strong",{parentName:"h4"},"Output")),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://i.imgur.com/cCuXiWe.jpg",alt:null})),(0,r.kt)("p",null,"In this example we will be running stable diffusion using on a GPU using bacalhau"),(0,r.kt)("p",null,"The source of this example is this ",(0,r.kt)("a",{parentName:"p",href:"https://colab.research.google.com/drive/1zVTa4mLeM_w44WaFwl7utTaa6JcaH1zK?usp=sharing"},"notebook")),(0,r.kt)("h1",{id:"keras-stable-diffusion-gpu-starter-example"},"Keras Stable Diffusion: GPU starter example"),(0,r.kt)("h2",{id:"install-gpu-requirements"},"Install GPU requirements"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"pip install git+https://github.com/fchollet/stable-diffusion-tensorflow --upgrade --quiet\npip install tensorflow tensorflow_addons ftfy --upgrade --quiet\npip install tqdm\napt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"  Building wheel for stable-diffusion-tf (setup.py) ... \x1b[?25l\x1b[?25hdone\n\x1b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 578.0 MB 17 kB/s \n\x1b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1 MB 15.4 MB/s \n\x1b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 53 kB 1.9 MB/s \n\x1b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.9 MB 56.0 MB/s \n\x1b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.7 MB 59.2 MB/s \n\x1b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 438 kB 70.1 MB/s \nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following package was automatically installed and is no longer required:\n  libnvidia-common-460\nUse 'apt autoremove' to remove it.\nThe following packages will be REMOVED:\n  libcudnn8-dev\nThe following held packages will be changed:\n  libcudnn8\nThe following packages will be upgraded:\n  libcudnn8\n1 upgraded, 0 newly installed, 1 to remove and 18 not upgraded.\nNeed to get 430 MB of archives.\nAfter this operation, 3,139 MB disk space will be freed.\nGet:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcudnn8 8.1.0.77-1+cuda11.2 [430 MB]\nFetched 430 MB in 13s (33.8 MB/s)\n(Reading database ... 159447 files and directories currently installed.)\nRemoving libcudnn8-dev (8.0.5.39-1+cuda11.1) ...\n(Reading database ... 159425 files and directories currently installed.)\nPreparing to unpack .../libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb ...\nUnpacking libcudnn8 (8.1.0.77-1+cuda11.2) over (8.0.5.39-1+cuda11.1) ...\nSetting up libcudnn8 (8.1.0.77-1+cuda11.2) ...\n")),(0,r.kt)("h2",{id:"lets-instantiate-a-text2image-generator-and-make-a-first-image"},"Let's instantiate a Text2Image generator and make a first image"),(0,r.kt)("p",null,"The first run has a bit of extra compilation overhead."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from stable_diffusion_tf.stable_diffusion import Text2Image\nfrom PIL import Image\n\ngenerator = Text2Image( \n    img_height=512,\n    img_width=512,\n    jit_compile=False,  # You can try True as well (different performance profile)\n)\nimg = generator.generate(\n    "DSLR photograph of an astronaut riding a horse",\n    num_steps=50,\n    unconditional_guidance_scale=7.5,\n    temperature=1,\n    batch_size=1,\n)\npil_img = Image.fromarray(img[0])\ndisplay(pil_img)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Downloading data from https://github.com/openai/CLIP/blob/main/clip/bpe_simple_vocab_16e6.txt.gz?raw=true\n1356917/1356917 [==============================] - 0s 0us/step\nDownloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/text_encoder.h5\n492456896/492456896 [==============================] - 6s 0us/step\nDownloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/diffusion_model.h5\n3439035312/3439035312 [==============================] - 48s 0us/step\nDownloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/decoder.h5\n198152112/198152112 [==============================] - 1s 0us/step\n\n\n  0   1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [01:10<00:00,  1.41s/it]\n")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"png",src:n(347).Z,width:"512",height:"512"})),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"pip install numba\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# clearing the GPU memory \nfrom numba import cuda \ndevice = cuda.get_current_device()\ndevice.reset()\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (0.56.2)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba) (4.12.0)\nRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba) (0.39.1)\nRequirement already satisfied: numpy<1.24,>=1.18 in /usr/local/lib/python3.7/dist-packages (from numba) (1.21.6)\nRequirement already satisfied: setuptools<60 in /usr/local/lib/python3.7/dist-packages (from numba) (57.4.0)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba) (3.8.1)\nRequirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba) (4.1.1)\n")),(0,r.kt)("p",null,"We will write a script that can take arguments and pass it to the stable diffusion generator"),(0,r.kt)("p",null,"That generates images and save the outputs as images"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'%%writefile stable-diffusion.py\nimport argparse\nfrom stable_diffusion_tf.stable_diffusion import Text2Image\nfrom PIL import Image\nimport os\nparser = argparse.ArgumentParser(description="Stable Diffusion")\nparser.add_argument("--h",dest="height", type=int,help="height of the image",default=512)\nparser.add_argument("--w",dest="width", type=int,help="width of the image",default=512)\nparser.add_argument("--p",dest="prompt", type=str,help="Description of the image you want to generate",default="cat")\nparser.add_argument("--n",dest="numSteps", type=int,help="Number of Steps",default=50)\nparser.add_argument("--u",dest="unconditionalGuidanceScale", type=float,help="Number of Steps",default=7.5)\nparser.add_argument("--t",dest="temperature", type=int,help="Number of Steps",default=1)\nparser.add_argument("--b",dest="batchSize", type=int,help="Number of Images",default=1)\nparser.add_argument("--o",dest="output", type=str,help="Output Folder where to store the Image",default="./")\n\nargs=parser.parse_args()\nheight=args.height\nwidth=args.width\nprompt=args.prompt\nnumSteps=args.numSteps\nunconditionalGuidanceScale=args.unconditionalGuidanceScale\ntemperature=args.temperature\nbatchSize=args.batchSize\noutput=args.output\n\ngenerator = Text2Image(\n    img_height=height,\n    img_width=width,\n    jit_compile=False,  # You can try True as well (different performance profile)\n)\n\nimg = generator.generate(\n    prompt,\n    num_steps=numSteps,\n    unconditional_guidance_scale=unconditionalGuidanceScale,\n    temperature=temperature,\n    batch_size=batchSize,\n)\nfor i in range(0,batchSize):\n  pil_img = Image.fromarray(img[i])\n  image = pil_img.save(f"{output}/image{i}.png")\n\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Overwriting stable-diffusion.py\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"python stable-diffusion.py\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"2022-09-29 15:57:32.473158: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2022-09-29 15:57:33.475937: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n2022-09-29 15:57:33.476158: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n2022-09-29 15:57:33.476182: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n2022-09-29 15:57:37.010234: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n 49 981:   0% 0/50 [00:00<?, ?it/s]2022-09-29 15:58:19.893237: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.48GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n2022-09-29 15:58:19.893362: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.48GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n2022-09-29 15:58:20.270785: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.69GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n2022-09-29 15:58:20.270859: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.69GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n2022-09-29 15:58:20.386111: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.67GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n2022-09-29 15:58:20.386176: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.67GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n2022-09-29 15:58:20.493172: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.67GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n2022-09-29 15:58:20.493253: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.67GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n2022-09-29 15:58:20.581118: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.67GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n2022-09-29 15:58:20.581185: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.67GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n  0   1: 100% 50/50 [01:05<00:00,  1.30s/it]\n")),(0,r.kt)("p",null,"Viewing the outputted image"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'import IPython.display as display\ndisplay.Image("image0.png")\n')),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"png",src:n(9127).Z,width:"512",height:"512"})),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"optional arguments:\n  -h, --help            show this help message and exit\n  --h HEIGHT            height of the image\n  --w WIDTH             width of the image\n  --p PROMPT            Description of the image you want to generate\n  --n NUMSTEPS          Number of Steps\n  --u UNCONDITIONALGUIDANCESCALE\n                        UNCONDITIONALGUIDANCESCALE\n  --t TEMPERATURE       Temparature\n  --b BATCHSIZE         Number of Images to generate\n  --o OUTPUT            Output Folder where to store the Image\n")),(0,r.kt)("h3",{id:"running-the-script-with-arguments"},"Running the script with arguments"),(0,r.kt)("h4",{id:"prompt-1"},"Prompt"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},'python stable-diffusion.py --p "cat with three eyes"')),(0,r.kt)("h4",{id:"number-of-iterations"},"Number of iterations"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'python stable-diffusion.py --p "cat with three eyes" --n 100\n')),(0,r.kt)("h4",{id:"batch-size-no-of-images-to-generate"},"Batch Size (No of images to generate)"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'python stable-diffusion.py --p "cat with three eyes" --b 2\n')),(0,r.kt)("p",null,"After that we will write a DOCKERFILE to containernize this script and then run it on bacalhau"),(0,r.kt)("h2",{id:"building-and-running-on-docker"},(0,r.kt)("strong",{parentName:"h2"},"Building and Running on docker")),(0,r.kt)("p",null,"In this step you will create a  ",(0,r.kt)("inlineCode",{parentName:"p"},"Dockerfile")," to create your Docker deployment. The ",(0,r.kt)("inlineCode",{parentName:"p"},"Dockerfile")," is a text document that contains the commands used to assemble the image."),(0,r.kt)("p",null,"First, create the ",(0,r.kt)("inlineCode",{parentName:"p"},"Dockerfile"),"."),(0,r.kt)("p",null,"Next, add your desired configuration to the ",(0,r.kt)("inlineCode",{parentName:"p"},"Dockerfile"),". These commands specify how the image will be built, and what extra requirements will be included."),(0,r.kt)("p",null,"Dockerfile"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"FROM tensorflow/tensorflow:latest-gpu\n\nRUN apt-get -y update\n\nRUN apt-get -y install git\n\nRUN python3 -m pip install --upgrade pip\n\nRUN python -m pip install regex tqdm Pillow\n\nRUN pip install git+https://github.com/fchollet/stable-diffusion-tensorflow --upgrade --quiet\n\nRUN pip install tensorflow tensorflow_addons ftfy --upgrade --quiet\n\nRUN apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n\nADD stable-diffusion.py stable-diffusion.py\n\nRUN python stable-diffusion.py --n 5\n")),(0,r.kt)("p",null,"In the dockerfile we will be using the tensorflow GPU image and then installing dependencies like git and other python python "),(0,r.kt)("p",null,"To Build the docker container run the docker build command"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"docker build -t <hub-user>/<repo-name>:<tag> .\n")),(0,r.kt)("p",null,"Please replace"),(0,r.kt)("p",null,"<","hub-user> with your docker hub username, If you don\u2019t have a docker hub account ",(0,r.kt)("a",{parentName:"p",href:"https://docs.docker.com/docker-id/"},"Follow these instructions to create docker account"),", and use the username of the account you created"),(0,r.kt)("p",null,"<","repo-name> This is the name of the container, you can name it anything you want"),(0,r.kt)("p",null,"<","tag> This is not required but you can use the latest tag"),(0,r.kt)("p",null,"After you have build the container, the next step is to test it locally and then push it docker hub"),(0,r.kt)("p",null,"Now you can push this repository to the registry designated by its name or tag."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"}," docker push <hub-user>/<repo-name>:<tag>\n")),(0,r.kt)("p",null,"After the repo image has been pushed to docker hub, we can now use the container for running on bacalhau"),(0,r.kt)("h2",{id:"running-the-container-on-bacalhau"},(0,r.kt)("strong",{parentName:"h2"},"Running the container on bacalhau")),(0,r.kt)("p",null,"After the repo image has been pushed to docker hub, we can now use the container for running on bacalhau"),(0,r.kt)("p",null,"We use the --gpu flag to denote the no of GPU we are going to use"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"bacalhau docker run \\\n--gpu 1 \\\njsacex/stable-diffusion-keras \\\n-- python stable-diffusion.py --o ./outputs\n")),(0,r.kt)("p",null,"Insalling bacalhau"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"curl -sL https://get.bacalhau.org/install.sh | bash\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Your system is linux_amd64\n\nBACALHAU CLI is detected:\nClient Version: v0.2.5\nServer Version: v0.2.5\nReinstalling BACALHAU CLI - /usr/local/bin/bacalhau...\nGetting the latest BACALHAU CLI...\nInstalling v0.2.5 BACALHAU CLI...\nDownloading https://github.com/filecoin-project/bacalhau/releases/download/v0.2.5/bacalhau_v0.2.5_linux_amd64.tar.gz ...\nDownloading sig file https://github.com/filecoin-project/bacalhau/releases/download/v0.2.5/bacalhau_v0.2.5_linux_amd64.tar.gz.signature.sha256 ...\nVerified OK\nExtracting tarball ...\nNOT verifying Bin\nbacalhau installed into /usr/local/bin successfully.\nClient Version: v0.2.5\nServer Version: v0.2.5\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"echo $(bacalhau docker run --id-only --wait --wait-timeout-secs 1000 --gpu 1 jsacex/stable-diffusion-keras -- python stable-diffusion.py --o ./outputs) > job_id.txt\ncat job_id.txt\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"4f758052-0543-40b5-bd86-6ab41e77389a\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"bacalhau list --id-filter $(cat job_id.txt)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"\x1b[92;100m CREATED  \x1b[0m\x1b[92;100m ID       \x1b[0m\x1b[92;100m JOB                     \x1b[0m\x1b[92;100m STATE     \x1b[0m\x1b[92;100m VERIFIED \x1b[0m\x1b[92;100m PUBLISHED               \x1b[0m\n\x1b[97;40m 17:33:46 \x1b[0m\x1b[97;40m 4f758052 \x1b[0m\x1b[97;40m Docker jsacex/stable... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmcQEQPg934Pow... \x1b[0m\n")),(0,r.kt)("p",null,'Where it says "',(0,r.kt)("inlineCode",{parentName:"p"},"Completed "),'", that means the job is done, and we can get the results.'),(0,r.kt)("p",null,"To find out more information about your job, run the following command:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"bacalhau describe $(cat job_id.txt)\n")),(0,r.kt)("p",null,"Since there is no error we can\u2019t see any error instead we see the state of our job to be complete, that means\nwe can download the results!\nwe create a temporary directory to save our results"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"mkdir results\n")),(0,r.kt)("p",null,"To Download the results of your job, run "),(0,r.kt)("hr",null),(0,r.kt)("p",null,"the following command:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"bacalhau get  $(cat job_id.txt)  --output-dir results\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"\x1b[90m17:38:25.343 |\x1b[0m \x1b[32mINF\x1b[0m \x1b[1mbacalhau/get.go:67\x1b[0m\x1b[36m >\x1b[0m Fetching results of job '4f758052-0543-40b5-bd86-6ab41e77389a'...\n2022/09/29 17:38:25 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 2048 kiB, got: 416 kiB). See https://github.com/lucas-clemente/quic-go/wiki/UDP-Receive-Buffer-Size for details.\n\x1b[90m17:38:35.851 |\x1b[0m \x1b[32mINF\x1b[0m \x1b[1mipfs/downloader.go:115\x1b[0m\x1b[36m >\x1b[0m Found 1 result shards, downloading to temporary folder.\n\x1b[90m17:38:37.1 |\x1b[0m \x1b[32mINF\x1b[0m \x1b[1mipfs/downloader.go:195\x1b[0m\x1b[36m >\x1b[0m Combining shard from output volume 'outputs' to final location: '/content/results'\n")),(0,r.kt)("p",null,"After the download has finished you should\nsee the following contents in results directory"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"ls results/\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"shards  stderr  stdout  volumes\n")),(0,r.kt)("p",null,"By Inspecting the Downloaded Results"),(0,r.kt)("p",null,"We can find that our generated image is located in /volumes/outputs/mars.png"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},".\n\u251c\u2500\u2500 shards\n\u2502   \u2514\u2500\u2500 job-2c281c1b-1a3e-4863-830f-8c48d117f6ea-shard-0-host-QmdZQ7ZbhnvWY1J12XYKGHApJ6aufKyLNSvf8jZBrBaAVL\n\u2502       \u251c\u2500\u2500 exitCode\n\u2502       \u251c\u2500\u2500 stderr\n\u2502       \u2514\u2500\u2500 stdout\n\u251c\u2500\u2500 stderr\n\u251c\u2500\u2500 stdout\n\u2514\u2500\u2500 volumes\n    \u2514\u2500\u2500 outputs\n        \u2514\u2500\u2500 cat.png\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'import IPython.display as display\ndisplay.Image("results/volumes/outputs/image0.png")\n')),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"png",src:n(7748).Z,width:"512",height:"512"})),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"bacalhau describe $(cat job_id.txt) --spec > job.yaml\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cat job.yaml\n")))}p.isMDXComponent=!0},9127:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/index_14_0-3c59a306b474849b6cbd490dceac49de.png"},7748:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/index_32_0-afaac739f343706619e768a0ce9907b6.png"},347:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/index_7_2-2408095b6bf7806c93c79caf89d76619.png"}}]);