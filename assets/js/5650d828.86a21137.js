"use strict";(self.webpackChunkbacalhau_docs=self.webpackChunkbacalhau_docs||[]).push([[2061],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>u});var n=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function s(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?s(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):s(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},s=Object.keys(e);for(n=0;n<s.length;n++)a=s[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(n=0;n<s.length;n++)a=s[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},d=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},b=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,s=e.originalType,l=e.parentName,d=i(e,["components","mdxType","originalType","parentName"]),b=c(a),u=o,p=b["".concat(l,".").concat(u)]||b[u]||m[u]||s;return a?n.createElement(p,r(r({ref:t},d),{},{components:a})):n.createElement(p,r({ref:t},d))}));function u(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var s=a.length,r=new Array(s);r[0]=b;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i.mdxType="string"==typeof e?e:o,r[1]=i;for(var c=2;c<s;c++)r[c]=a[c];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}b.displayName="MDXCreateElement"},2073:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>s,metadata:()=>i,toc:()=>c});var n=a(7462),o=(a(7294),a(3905));const s={sidebar_label:"Ethereum Blockchain Analysis",sidebar_position:3},r="Ethereum Blockchain Analysis with Ethereum-ETL and Bacalhau",i={unversionedId:"examples/data-engineering/blockchain-etl/index",id:"examples/data-engineering/blockchain-etl/index",title:"Ethereum Blockchain Analysis with Ethereum-ETL and Bacalhau",description:"Open In Colab",source:"@site/docs/examples/data-engineering/blockchain-etl/index.md",sourceDirName:"examples/data-engineering/blockchain-etl",slug:"/examples/data-engineering/blockchain-etl/",permalink:"/examples/data-engineering/blockchain-etl/",draft:!1,editUrl:"https://github.com/bacalhau-project/docs.bacalhau.org/blob/main/docs/examples/data-engineering/blockchain-etl/index.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_label:"Ethereum Blockchain Analysis",sidebar_position:3},sidebar:"documentationSidebar",previous:{title:"Simple Parallel Workloads",permalink:"/examples/data-engineering/simple-parallel-workloads/"},next:{title:"Oceanography - Data Conversion",permalink:"/examples/data-engineering/oceanography-conversion/"}},l={},c=[{value:"Prerequisites",id:"prerequisites",level:3},{value:"1. Analysing Ethereum Data Locally",id:"1-analysing-ethereum-data-locally",level:2},{value:"2. Analysing Ethereum Data With Bacalhau",id:"2-analysing-ethereum-data-with-bacalhau",level:2},{value:"Analysing Ethereum Data On Bacalhau",id:"analysing-ethereum-data-on-bacalhau",level:3},{value:"Massive Scale Ethereum Analysis",id:"massive-scale-ethereum-analysis",level:3},{value:"Appendix 1: List Ethereum Data CIDs",id:"appendix-1-list-ethereum-data-cids",level:2},{value:"Appendix 2: Setting up an Ethereum Node",id:"appendix-2-setting-up-an-ethereum-node",level:2},{value:"Geth setup and sync",id:"geth-setup-and-sync",level:3},{value:"Extracting the Data",id:"extracting-the-data",level:3},{value:"Upload the data",id:"upload-the-data",level:3}],d={toc:c};function m(e){let{components:t,...s}=e;return(0,o.kt)("wrapper",(0,n.Z)({},d,s,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"ethereum-blockchain-analysis-with-ethereum-etl-and-bacalhau"},"Ethereum Blockchain Analysis with Ethereum-ETL and Bacalhau"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://colab.research.google.com/github/bacalhau-project/examples/blob/main/data-engineering/blockchain-etl/index.ipynb"},(0,o.kt)("img",{parentName:"a",src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})),"\n",(0,o.kt)("a",{parentName:"p",href:"https://mybinder.org/v2/gh/bacalhau-project/examples/HEAD?labpath=data-engineering/blockchain-etl/index.ipynb"},(0,o.kt)("img",{parentName:"a",src:"https://mybinder.org/badge.svg",alt:"Open In Binder"}))),(0,o.kt)("h1",{id:"introduction"},"Introduction"),(0,o.kt)("p",null,"Mature blockchains are difficult to analyze because of their size. Ethereum-ETL is a tool that makes it easy to extract information from an Ethereum node, but it's not easy to get working in a batch manner. It takes approximately 1 week for an Ethereum node to download the entire chain (event more in my experience) and importing and exporting data from the Ethereum node is slow."),(0,o.kt)("p",null,"For this example, we ran an Ethereum node for a week and allowed it to synchronise. We then ran ethereum-etl to extract the information and pinned it on Filecoin. This means that we can both now access the data without having to run another ethereum node."),(0,o.kt)("p",null,"But there's still a lot of data and these types of analyses typically need repeating or refining. So it makes absolute sense to use a decentralised network like Bacalhau to process the data in a scalable way."),(0,o.kt)("h3",{id:"prerequisites"},"Prerequisites"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Python 3 "),(0,o.kt)("li",{parentName:"ul"},"The Bacalhau client - ",(0,o.kt)("a",{parentName:"li",href:"https://docs.bacalhau.org/getting-started/installation"},"Installation instructions"))),(0,o.kt)("h2",{id:"1-analysing-ethereum-data-locally"},"1. Analysing Ethereum Data Locally"),(0,o.kt)("p",null,"First let's download one of the IPFS files and inspect it locally. You can see the full list of IPFS CIDs in the appendix."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"wget -q -O file.tar.gz https://w3s.link/ipfs/bafybeifgqjvmzbtz427bne7af5tbndmvniabaex77us6l637gqtb2iwlwq\ntar -xvf file.tar.gz\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"x output_850000/\nx output_850000/token_transfers/\nx output_850000/token_transfers/start_block=00850000/\nx output_850000/token_transfers/start_block=00850000/end_block=00899999/\nx output_850000/token_transfers/start_block=00850000/end_block=00899999/token_transfers_00850000_00899999.csv\nx output_850000/contracts/\nx output_850000/contracts/start_block=00850000/\nx output_850000/contracts/start_block=00850000/end_block=00899999/\nx output_850000/contracts/start_block=00850000/end_block=00899999/contracts_00850000_00899999.csv\nx output_850000/transactions/\nx output_850000/transactions/start_block=00850000/\nx output_850000/transactions/start_block=00850000/end_block=00899999/\nx output_850000/transactions/start_block=00850000/end_block=00899999/transactions_00850000_00899999.csv\nx output_850000/receipts/\nx output_850000/receipts/start_block=00850000/\nx output_850000/receipts/start_block=00850000/end_block=00899999/\nx output_850000/receipts/start_block=00850000/end_block=00899999/receipts_00850000_00899999.csv\nx output_850000/tokens/\nx output_850000/tokens/start_block=00850000/\nx output_850000/tokens/start_block=00850000/end_block=00899999/\nx output_850000/tokens/start_block=00850000/end_block=00899999/tokens_00850000_00899999.csv\nx output_850000/blocks/\nx output_850000/blocks/start_block=00850000/\nx output_850000/blocks/start_block=00850000/end_block=00899999/\nx output_850000/blocks/start_block=00850000/end_block=00899999/blocks_00850000_00899999.csv\nx output_850000/.tmp/\nx output_850000/logs/\nx output_850000/logs/start_block=00850000/\nx output_850000/logs/start_block=00850000/end_block=00899999/\nx output_850000/logs/start_block=00850000/end_block=00899999/logs_00850000_00899999.csv\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip install pandas\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Requirement already satisfied: pandas in /Users/phil/.pyenv/versions/3.9.7/lib/python3.9/site-packages (1.4.3)\nRequirement already satisfied: python-dateutil>=2.8.1 in /Users/phil/.local/lib/python3.9/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: numpy>=1.20.0 in /Users/phil/.pyenv/versions/3.9.7/lib/python3.9/site-packages (from pandas) (1.23.0)\nRequirement already satisfied: pytz>=2020.1 in /Users/phil/.local/lib/python3.9/site-packages (from pandas) (2021.1)\nRequirement already satisfied: six>=1.5 in /Users/phil/.pyenv/versions/3.9.7/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# Use pandas to read in transation data and clean up the columns\nimport pandas as pd\nimport glob\n\nfile = glob.glob('output_*/transactions/start_block=*/end_block=*/transactions*.csv')[0]\nprint(\"Loading file %s\" % file)\ndf = pd.read_csv(file)\ndf['value'] = df['value'].astype('float')\ndf['from_address'] = df['from_address'].astype('string')\ndf['to_address'] = df['to_address'].astype('string')\ndf['hash'] = df['hash'].astype('string')\ndf['block_hash'] = df['block_hash'].astype('string')\ndf['block_datetime'] = pd.to_datetime(df['block_timestamp'], unit='s')\ndf.info()\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Loading file output_850000/transactions/start_block=00850000/end_block=00899999/transactions_00850000_00899999.csv\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 115517 entries, 0 to 115516\nData columns (total 16 columns):\n #   Column                    Non-Null Count   Dtype         \n---  ------                    --------------   -----         \n 0   hash                      115517 non-null  string        \n 1   nonce                     115517 non-null  int64         \n 2   block_hash                115517 non-null  string        \n 3   block_number              115517 non-null  int64         \n 4   transaction_index         115517 non-null  int64         \n 5   from_address              115517 non-null  string        \n 6   to_address                114901 non-null  string        \n 7   value                     115517 non-null  float64       \n 8   gas                       115517 non-null  int64         \n 9   gas_price                 115517 non-null  int64         \n 10  input                     115517 non-null  object        \n 11  block_timestamp           115517 non-null  int64         \n 12  max_fee_per_gas           0 non-null       float64       \n 13  max_priority_fee_per_gas  0 non-null       float64       \n 14  transaction_type          115517 non-null  int64         \n 15  block_datetime            115517 non-null  datetime64[ns]\ndtypes: datetime64[ns](1), float64(3), int64(7), object(1), string(4)\nmemory usage: 14.1+ MB\n")),(0,o.kt)("p",null,"The following code inspects the daily trading volume of Ethereum for a single chunk (100,000 blocks) of data."),(0,o.kt)("p",null,"This is all good, but we can do better. We can use the Bacalhau client to download the data from IPFS and then run the analysis on the data in the cloud. This means that we can analyse the entire Ethereum blockchain without having to download it locally."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# Total volume per day\ndf[['block_datetime', 'value']].groupby(pd.Grouper(key='block_datetime', freq='1D')).sum().plot()\n\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"<AxesSubplot:xlabel='block_datetime'>\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"png",src:a(6901).Z,width:"378",height:"297"})),(0,o.kt)("h2",{id:"2-analysing-ethereum-data-with-bacalhau"},"2. Analysing Ethereum Data With Bacalhau"),(0,o.kt)("p",null,"To run jobs on the Bacalhau network you need to package your code. In this example I will package the code as a Docker image."),(0,o.kt)("p",null,"But before we do that, we need to develop the code that will perform the analysis. The code below is a simple script to parse the incoming data and produce a CSV file with the daily trading volume of Ethereum."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'%%writefile main.py\nimport glob, os, sys, shutil, tempfile\nimport pandas as pd\n\ndef main(input_dir, output_dir):\n    search_path = os.path.join(input_dir, "output*", "transactions", "start_block*", "end_block*", "transactions_*.csv")\n    csv_files = glob.glob(search_path)\n    if len(csv_files) == 0:\n        print("No CSV files found in %s" % search_path)\n        sys.exit(1)\n    for transactions_file in csv_files:\n        print("Loading %s" % transactions_file)\n        df = pd.read_csv(transactions_file)\n        df[\'value\'] = df[\'value\'].astype(\'float\')\n        df[\'block_datetime\'] = pd.to_datetime(df[\'block_timestamp\'], unit=\'s\')\n        \n        print("Processing %d blocks" % (df.shape[0]))\n        results = df[[\'block_datetime\', \'value\']].groupby(pd.Grouper(key=\'block_datetime\', freq=\'1D\')).sum()\n        print("Finished processing %d days worth of records" % (results.shape[0]))\n\n        save_path = os.path.join(output_dir, os.path.basename(transactions_file))\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        print("Saving to %s" % (save_path))\n        results.to_csv(save_path)\n\ndef extractData(input_dir, output_dir):\n    search_path = os.path.join(input_dir, "*.tar.gz")\n    gz_files = glob.glob(search_path)\n    if len(gz_files) == 0:\n        print("No tar.gz files found in %s" % search_path)\n        sys.exit(1)\n    for f in gz_files:\n        shutil.unpack_archive(filename=f, extract_dir=output_dir)\n\nif __name__ == "__main__":\n    if len(sys.argv) != 3:\n        print(\'Must pass arguments. Format: [command] input_dir output_dir\')\n        sys.exit()\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        extractData(sys.argv[1], tmp_dir)\n        main(tmp_dir, sys.argv[2])\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Writing main.py\n")),(0,o.kt)("p",null,"Next, let's make sure the file works as expected..."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"python main.py . outputs/\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Loading /var/folders/kr/pl4p96k11b55hp5_p9l_t8kr0000gn/T/tmphtgurgu0/output_850000/transactions/start_block=00850000/end_block=00899999/transactions_00850000_00899999.csv\nProcessing 115517 blocks\nFinished processing 11 days worth of records\nSaving to outputs/transactions_00850000_00899999.csv\n")),(0,o.kt)("p",null,"And finally, package the code inside a Docker image to make the process reproducible. Here I'm passing the Bacalhau default ",(0,o.kt)("inlineCode",{parentName:"p"},"/inputs")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"/outputs")," directories. The ",(0,o.kt)("inlineCode",{parentName:"p"},"/inputs")," directory is where the data will be read from and the ",(0,o.kt)("inlineCode",{parentName:"p"},"/outputs")," directory is where the results will be saved to."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'%%writefile Dockerfile\nFROM python:3.11-slim-bullseye\nWORKDIR /src\nRUN pip install pandas==1.5.1\nADD main.py .\nCMD ["python", "main.py", "/inputs", "/outputs"]\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Overwriting Dockerfile\n")),(0,o.kt)("p",null,"We've already pushed the container, but for posterity, the following command pushes this container to GHCR."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"docker buildx build --platform linux/amd64 --push -t ghcr.io/bacalhau-project/examples/blockchain-etl:0.0.1 .\n")),(0,o.kt)("h3",{id:"analysing-ethereum-data-on-bacalhau"},"Analysing Ethereum Data On Bacalhau"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://www.bacalhau.org/"},"Bacalhau")," is a distributed computing platform that allows you to run jobs on a network of computers. It is designed to be easy to use and to run on a variety of hardware. In this example, we will use it to run our analysis on the Ethereum blockchain."),(0,o.kt)("p",null,"To submit a job, you can use the Bacalhau CLI. The following command will run the container above on the IPFS data -- the long hash -- shown at the start of this notebook. Let's confirm that the results are as expected."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"bacalhau docker run \\\n    --id-only \\\n    --input-volumes bafybeifgqjvmzbtz427bne7af5tbndmvniabaex77us6l637gqtb2iwlwq:/inputs/data.tar.gz \\\n    ghcr.io/bacalhau-project/examples/blockchain-etl:0.0.6\n")),(0,o.kt)("p",null,"Running the commands will output a UUID that represents the job that was created. You can check the status of the job with the following command:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"bacalhau list --id-filter ${JOB_ID}\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"\x1b[92;100m CREATED  \x1b[0m\x1b[92;100m ID       \x1b[0m\x1b[92;100m JOB                     \x1b[0m\x1b[92;100m STATE     \x1b[0m\x1b[92;100m VERIFIED \x1b[0m\x1b[92;100m PUBLISHED               \x1b[0m\n\x1b[97;40m 11:35:52 \x1b[0m\x1b[97;40m 5466b0ae \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmeaJY77DrN2KD... \x1b[0m\n")),(0,o.kt)("p",null,"Wait until it says ",(0,o.kt)("inlineCode",{parentName:"p"},"Completed")," and then get the results."),(0,o.kt)("p",null,"To find out more information about your job, run the following command:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"bacalhau describe ${JOB_ID}\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'APIVersion: V1beta1\nClientID: 77cf46c04f88ffb1c3e0e4b6e443724e8d2d87074d088ef1a6294a448fa85d2e\nCreatedAt: "2022-11-21T11:35:52.834067363Z"\nDeal:\n  Concurrency: 1\nExecutionPlan:\n  ShardsTotal: 1\nID: 5466b0ae-4eca-429c-a7b7-a9a5eeb0268c\nJobState:\n  Nodes:\n    QmSyJ8VUd4YSPwZFJSJsHmmmmg7sd4BAc2yHY73nisJo86:\n      Shards:\n        "0":\n          NodeId: QmSyJ8VUd4YSPwZFJSJsHmmmmg7sd4BAc2yHY73nisJo86\n          PublishedResults: {}\n          State: Cancelled\n          VerificationResult: {}\n    QmVAb7r2pKWCuyLpYWoZr9syhhFnTWeFaByHdb8PkkhLQG:\n      Shards:\n        "0":\n          NodeId: QmVAb7r2pKWCuyLpYWoZr9syhhFnTWeFaByHdb8PkkhLQG\n          PublishedResults: {}\n          State: Cancelled\n          VerificationResult: {}\n    QmXaXu9N5GNetatsvwnTfQqNtSeKAD6uCmarbh3LMRYAcF:\n      Shards:\n        "0":\n          NodeId: QmXaXu9N5GNetatsvwnTfQqNtSeKAD6uCmarbh3LMRYAcF\n          PublishedResults:\n            CID: QmeaJY77DrN2KDMcuyVofEzkXggnWZYEqi6ePXKAbdkavf\n            Name: job-5466b0ae-4eca-429c-a7b7-a9a5eeb0268c-shard-0-host-QmXaXu9N5GNetatsvwnTfQqNtSeKAD6uCmarbh3LMRYAcF\n            StorageSource: IPFS\n          RunOutput:\n            exitCode: 0\n            runnerError: ""\n            stderr: ""\n            stderrtruncated: false\n            stdout: |\n              Loading /tmp/tmpjgrd_43o/output_850000/transactions/start_block=00850000/end_block=00899999/transactions_00850000_00899999.csv\n              Processing 115517 blocks\n              Finished processing 11 days worth of records\n              Saving to /outputs/transactions_00850000_00899999.csv\n            stdouttruncated: false\n          State: Completed\n          Status: \'Got results proposal of length: 0\'\n          VerificationResult:\n            Complete: true\n            Result: true\nRequesterNodeID: QmdZQ7ZbhnvWY1J12XYKGHApJ6aufKyLNSvf8jZBrBaAVL\nRequesterPublicKey: CAASpgIwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDVRKPgCfY2fgfrkHkFjeWcqno+MDpmp8DgVaY672BqJl/dZFNU9lBg2P8Znh8OTtHPPBUBk566vU3KchjW7m3uK4OudXrYEfSfEPnCGmL6GuLiZjLf+eXGEez7qPaoYqo06gD8ROdD8VVse27E96LlrpD1xKshHhqQTxKoq1y6Rx4DpbkSt966BumovWJ70w+Nt9ZkPPydRCxVnyWS1khECFQxp5Ep3NbbKtxHNX5HeULzXN5q0EQO39UN6iBhiI34eZkH7PoAm3Vk5xns//FjTAvQw6wZUu8LwvZTaihs+upx2zZysq6CEBKoeNZqed9+Tf+qHow0P5pxmiu+or+DAgMBAAE=\nSpec:\n  Docker:\n    Image: ghcr.io/bacalhau-project/examples/blockchain-etl:0.0.6\n  Engine: Docker\n  Language:\n    JobContext: {}\n  Publisher: Estuary\n  Resources:\n    GPU: ""\n  Sharding:\n    BatchSize: 1\n    GlobPatternBasePath: /inputs\n  Timeout: 1800\n  Verifier: Noop\n  Wasm: {}\n  inputs:\n  - CID: bafybeifgqjvmzbtz427bne7af5tbndmvniabaex77us6l637gqtb2iwlwq\n    StorageSource: IPFS\n    path: /inputs/data.tar.gz\n  outputs:\n  - Name: outputs\n    StorageSource: IPFS\n    path: /outputs\n')),(0,o.kt)("p",null,"And let's inspect the results."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"rm -rf ./results && mkdir -p ./results # Temporary directory to store the results\nbacalhau get --output-dir ./results ${JOB_ID} # Download the results\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Fetching results of job '5466b0ae-4eca-429c-a7b7-a9a5eeb0268c'...\nResults for job '5466b0ae-4eca-429c-a7b7-a9a5eeb0268c' have been written to...\n./results\n")),(0,o.kt)("p",null,"The docker run command above used the ",(0,o.kt)("inlineCode",{parentName:"p"},"outputs")," volume as a results folder so when we download them they will be stored in a  folder within ",(0,o.kt)("inlineCode",{parentName:"p"},"volumes/outputs"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"ls -lah results/combined_results/outputs\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"total 4.0K\ndrwxr-xr-x 3 phil staff  96 Nov 21 11:36 .\ndrwxr-xr-x 5 phil staff 160 Nov 21 11:36 ..\n-rw-r--r-- 3 phil staff 387 Nov 21 11:36 transactions_00850000_00899999.csv\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import glob\nimport pandas as pd\n\n# Get CSV files list from a folder\ncsv_files = glob.glob(\"results/combined_results/outputs/*.csv\")\ndf = pd.read_csv(csv_files[0], index_col='block_datetime')\ndf.plot()\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"<AxesSubplot:xlabel='block_datetime'>\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"png",src:a(9663).Z,width:"385",height:"274"})),(0,o.kt)("h3",{id:"massive-scale-ethereum-analysis"},"Massive Scale Ethereum Analysis"),(0,o.kt)("p",null,"Ok so that works. Let's scale this up! We can run the same analysis on the entire Ethereum blockchain (up to the point where I have uploaded the Ethereum data). To do this, we need to run the analysis on each of the chunks of data that we have stored on IPFS. We can do this by running the same job on each of the chunks."),(0,o.kt)("p",null,"See the appendix for the ",(0,o.kt)("inlineCode",{parentName:"p"},"hashes.txt")," file."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'printf "" > job_ids.txt\nfor h in $(cat hashes.txt); do \\\n    bacalhau docker run \\\n    --id-only \\\n    --wait=false \\\n    --input-volumes=$h:/inputs/data.tar.gz \\\n    ghcr.io/bacalhau-project/examples/blockchain-etl:0.0.6 >> job_ids.txt \ndone\n')),(0,o.kt)("p",null,"Now take a look at the job id's. You can use these to check the status of the jobs and download the results. You might want to double check that the jobs ran ok by doing a ",(0,o.kt)("inlineCode",{parentName:"p"},"bacalhau list"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"cat job_ids.txt\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"7e7b9df1-4904-45f5-a392-1f99d1c3a4dd\ne3e5f500-0ebe-4948-a1b8-80d7b42dd534\n56fd0c21-99d7-4bf5-8178-d171c91f8576\nd026ff34-7ecd-42e0-8431-52d9497e9eb6\nab919292-74f3-493f-8d66-1f56c4afc819\nca6b0fbf-061d-4380-8c6e-faac5ec67a15\n9d05a2a4-9981-4763-8490-3b849eaa628b\nc5baf5cc-3259-4acf-ba75-92e1cb2235bb\n32f97eaa-cad6-49f0-9b20-a2a1276980be\n69bd1c82-1be7-4b1f-8ab0-d5d2bef19bb1\n31a43a56-fc31-4254-83b2-8fa8f8a6e542\necfdf8f8-dbad-4447-a275-af8fceb6abfc\nd45365d4-9cf3-4a24-9ae1-c8c8d8d5856f\n08cbab6e-88a4-4a20-bcd3-26e2c8f379a7\n93bcb94e-19a3-49ba-af18-ea6566515b93\nce6a8215-b5d4-4cb3-b4ac-e24ef1f84e1c\n354c3482-4f3a-4d56-b908-43bb8708450f\n7cc6e343-2b97-4826-9b73-b5bd32019ffc\n3d396eed-abb4-47bc-a4dc-24647c619db9\nbc2c6361-39dc-4970-8284-be1f7f011c66\nf421a3e8-009e-4205-898b-9811a78d6a36\nab73b414-edac-45f9-95e6-06a855629b23\n44711531-3aff-4b65-9db9-9fc6d3948023\nf6143867-f2ab-4701-a85a-375f080bcd3d\n6b8552f8-8505-4139-bb88-ab5e5e0b8cef\nd3e25508-7b28-4e83-be0d-be921dbfd7e9\n0f4dfdb6-bf13-41af-bc7a-e045ac360ba7\ncc16970e-a5be-42f4-bec5-098acbe4d7d9\n37819801-f09b-4d3c-86ce-070a4f8c37a3\nd55cc9e7-9e33-4485-9137-2b5c7e1e831a\n8313e946-e587-468a-83e1-e464138d3935\n65681fa0-44ee-46a4-ab46-6db705d1d318\nc164e617-01a9-4967-ac29-f2c5e2ddeb3f\n0e0b5e9b-d467-41f6-965e-249f06e7a32b\neb733f15-a129-499e-a59c-8eaba0dd68cc\nfcf637d3-1817-4a99-9884-9db568e4d18d\ne9450225-c2fb-47ea-b735-ebbde28bdd33\n2183006a-3992-4c7f-9200-c4c4e0cd69ab\nf86f2507-d408-4b70-8cdb-47f3ff2083a2\ncb0670d4-eaa5-4181-808d-3bc92da9b4b8\n868707f5-dfdc-48e3-ab44-f4d450922a0d\n2dfc0ce6-d685-424b-95ff-921ad5da0812\nd034fd07-e7fb-41c7-b8d3-5c9b1ea01fad\n3e40318b-ba7a-4d67-9dfb-217e4776c62a\na2e17933-5eb5-4866-bb01-a88fb9429608\nd1407c23-a485-43a3-87cf-9aeb28e656bf\n55ef4899-03af-4667-8c93-d00c01d8d657\n7085672f-0571-4a41-b5f7-a983fda9e2b1\n1ba46518-bf45-45e5-afa6-928844f98e5f\nede7c3a2-3cf1-4d08-abe3-278f55d0c27e\n7aa140ab-79c6-452f-a3f6-19035e7187cd\n31787f8f-a566-4b2f-b3f4-91390d3bea68\nb167a9f3-1c5f-40e8-b7c5-e864221deab9\nadab8249-2fb8-49b4-bc01-faac00810a26\n")),(0,o.kt)("p",null,"Wait until all of these jobs have completed:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"bacalhau list -n 50\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"\x1b[92;100m CREATED  \x1b[0m\x1b[92;100m ID       \x1b[0m\x1b[92;100m JOB                     \x1b[0m\x1b[92;100m STATE     \x1b[0m\x1b[92;100m VERIFIED \x1b[0m\x1b[92;100m PUBLISHED               \x1b[0m\n\x1b[97;40m 09:20:45 \x1b[0m\x1b[97;40m adab8249 \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmYhXreDR4LKBq... \x1b[0m\n\x1b[37;40m 09:20:44 \x1b[0m\x1b[37;40m b167a9f3 \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmVcmigC4RGJjM... \x1b[0m\n\x1b[97;40m 09:20:43 \x1b[0m\x1b[97;40m 7aa140ab \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmQahTAxdLWxCC... \x1b[0m\n\x1b[37;40m 09:20:43 \x1b[0m\x1b[37;40m 31787f8f \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmXtbqVopAhc7D... \x1b[0m\n\x1b[97;40m 09:20:42 \x1b[0m\x1b[97;40m ede7c3a2 \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmVHcd4HrnhALb... \x1b[0m\n\x1b[37;40m 09:20:41 \x1b[0m\x1b[37;40m 1ba46518 \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmWxxBym5Rssai... \x1b[0m\n\x1b[97;40m 09:20:41 \x1b[0m\x1b[97;40m 7085672f \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmX5vA9gCpKLMM... \x1b[0m\n\x1b[37;40m 09:20:40 \x1b[0m\x1b[37;40m 55ef4899 \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmV2ZQoyjFu1Bt... \x1b[0m\n\x1b[97;40m 09:20:39 \x1b[0m\x1b[97;40m d1407c23 \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmQafnmkoLP9fD... \x1b[0m\n\x1b[37;40m 09:20:39 \x1b[0m\x1b[37;40m a2e17933 \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmYJrU1xRyWZxn... \x1b[0m\n\x1b[97;40m 09:20:38 \x1b[0m\x1b[97;40m 3e40318b \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmSG4ZMP5m5mko... \x1b[0m\n\x1b[37;40m 09:20:37 \x1b[0m\x1b[37;40m 2dfc0ce6 \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmYEEGsQYqoz5D... \x1b[0m\n\x1b[97;40m 09:20:37 \x1b[0m\x1b[97;40m d034fd07 \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmRYZiQrdPZoXf... \x1b[0m\n\x1b[37;40m 09:20:36 \x1b[0m\x1b[37;40m 868707f5 \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmRUjZFGXqAiEH... \x1b[0m\n\x1b[97;40m 09:20:35 \x1b[0m\x1b[97;40m cb0670d4 \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmSjZx4gTdmbCf... \x1b[0m\n\x1b[37;40m 09:20:34 \x1b[0m\x1b[37;40m f86f2507 \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmVH9JdUDo5aPG... \x1b[0m\n\x1b[97;40m 09:20:34 \x1b[0m\x1b[97;40m 2183006a \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmR81jwpXVFJ6t... \x1b[0m\n\x1b[37;40m 09:20:33 \x1b[0m\x1b[37;40m e9450225 \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmSraHbxuprHTk... \x1b[0m\n\x1b[97;40m 09:20:32 \x1b[0m\x1b[97;40m eb733f15 \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmfV7e9iVDbSg6... \x1b[0m\n\x1b[37;40m 09:20:32 \x1b[0m\x1b[37;40m fcf637d3 \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmViFm5G8aeL7e... \x1b[0m\n\x1b[97;40m 09:20:31 \x1b[0m\x1b[97;40m 0e0b5e9b \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmaYobcWeo9XC8... \x1b[0m\n\x1b[37;40m 09:20:30 \x1b[0m\x1b[37;40m 65681fa0 \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmSec41PLJm6Ba... \x1b[0m\n\x1b[97;40m 09:20:30 \x1b[0m\x1b[97;40m c164e617 \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmRva4GxizU8pt... \x1b[0m\n\x1b[37;40m 09:20:29 \x1b[0m\x1b[37;40m 8313e946 \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmSC5Pd6arQVDA... \x1b[0m\n\x1b[97;40m 09:20:28 \x1b[0m\x1b[97;40m 37819801 \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmUG36RPRZXdop... \x1b[0m\n\x1b[37;40m 09:20:28 \x1b[0m\x1b[37;40m d55cc9e7 \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmZc2ThoFfoqwc... \x1b[0m\n\x1b[97;40m 09:20:27 \x1b[0m\x1b[97;40m cc16970e \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmSVm6z4Z9MD5t... \x1b[0m\n\x1b[37;40m 09:20:26 \x1b[0m\x1b[37;40m 0f4dfdb6 \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmW6bdwq3xWp8d... \x1b[0m\n\x1b[97;40m 09:20:25 \x1b[0m\x1b[97;40m d3e25508 \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmdKAxhWU527as... \x1b[0m\n\x1b[37;40m 09:20:25 \x1b[0m\x1b[37;40m 6b8552f8 \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmWAKGZ3rSDsY6... \x1b[0m\n\x1b[97;40m 09:20:24 \x1b[0m\x1b[97;40m f6143867 \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmPfcKczWGWH4p... \x1b[0m\n\x1b[37;40m 09:20:23 \x1b[0m\x1b[37;40m ab73b414 \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmPFjzJ7ZZnefs... \x1b[0m\n\x1b[97;40m 09:20:23 \x1b[0m\x1b[97;40m 44711531 \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmNvk3hE8mULBd... \x1b[0m\n\x1b[37;40m 09:20:22 \x1b[0m\x1b[37;40m f421a3e8 \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmWooZw9qMSMy1... \x1b[0m\n\x1b[97;40m 09:20:21 \x1b[0m\x1b[97;40m bc2c6361 \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmPRuHPd1htJNr... \x1b[0m\n\x1b[37;40m 09:20:21 \x1b[0m\x1b[37;40m 3d396eed \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmPPxMjvY4Tj5t... \x1b[0m\n\x1b[97;40m 09:20:20 \x1b[0m\x1b[97;40m 7cc6e343 \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmaPQDQpaACyeX... \x1b[0m\n\x1b[37;40m 09:20:19 \x1b[0m\x1b[37;40m ce6a8215 \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmWs9RzYBMGsZo... \x1b[0m\n\x1b[97;40m 09:20:19 \x1b[0m\x1b[97;40m 354c3482 \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmRoAYAQtufUc7... \x1b[0m\n\x1b[37;40m 09:20:18 \x1b[0m\x1b[37;40m 93bcb94e \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmRJ5jjUZ7jeoh... \x1b[0m\n\x1b[97;40m 09:20:17 \x1b[0m\x1b[97;40m 08cbab6e \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmRGC8upqEejib... \x1b[0m\n\x1b[37;40m 09:20:16 \x1b[0m\x1b[37;40m d45365d4 \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmU9FuJWEDWfZJ... \x1b[0m\n\x1b[97;40m 09:20:16 \x1b[0m\x1b[97;40m ecfdf8f8 \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmWmiM83zF7HwC... \x1b[0m\n\x1b[37;40m 09:20:15 \x1b[0m\x1b[37;40m 31a43a56 \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmPjfMpy1cK9Mf... \x1b[0m\n\x1b[97;40m 09:20:14 \x1b[0m\x1b[97;40m 32f97eaa \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmXUc75sJqJFYJ... \x1b[0m\n\x1b[37;40m 09:20:14 \x1b[0m\x1b[37;40m 69bd1c82 \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/Qma5gN7LGYeTC9... \x1b[0m\n\x1b[97;40m 09:20:13 \x1b[0m\x1b[97;40m c5baf5cc \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmPVJEx3CGZYKW... \x1b[0m\n\x1b[37;40m 09:20:12 \x1b[0m\x1b[37;40m 9d05a2a4 \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmNnPNY1K21345... \x1b[0m\n\x1b[97;40m 09:20:12 \x1b[0m\x1b[97;40m ca6b0fbf \x1b[0m\x1b[97;40m Docker ghcr.io/bacal... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmWj41DwL3iwH4... \x1b[0m\n\x1b[37;40m 09:20:11 \x1b[0m\x1b[37;40m ab919292 \x1b[0m\x1b[37;40m Docker ghcr.io/bacal... \x1b[0m\x1b[37;40m Completed \x1b[0m\x1b[37;40m          \x1b[0m\x1b[37;40m /ipfs/QmQwsvzpT9iKzQ... \x1b[0m\n")),(0,o.kt)("p",null,"And then download all the results and merge them into a single directory. This might take a while, so this is a good time to treat yourself to a nice Dark Mild. There's also been some issues in the past communicating with IPFS, so if you get an error, try again."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"rm -rf ./combined_results && mkdir -p ./combined_results\nfor id in $(cat job_ids.txt); do \\\n    rm -rf results && mkdir results\n    bacalhau get --output-dir ./results $id\n    cp results/combined_results/outputs/* ./combined_results\ndone\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Fetching results of job '7e7b9df1-4904-45f5-a392-1f99d1c3a4dd'...\nResults for job '7e7b9df1-4904-45f5-a392-1f99d1c3a4dd' have been written to...\n./results\nFetching results of job 'e3e5f500-0ebe-4948-a1b8-80d7b42dd534'...\nResults for job 'e3e5f500-0ebe-4948-a1b8-80d7b42dd534' have been written to...\n./results\nFetching results of job '56fd0c21-99d7-4bf5-8178-d171c91f8576'...\nResults for job '56fd0c21-99d7-4bf5-8178-d171c91f8576' have been written to...\n./results\nFetching results of job 'd026ff34-7ecd-42e0-8431-52d9497e9eb6'...\nResults for job 'd026ff34-7ecd-42e0-8431-52d9497e9eb6' have been written to...\n./results\nFetching results of job 'ab919292-74f3-493f-8d66-1f56c4afc819'...\nResults for job 'ab919292-74f3-493f-8d66-1f56c4afc819' have been written to...\n./results\nFetching results of job 'ca6b0fbf-061d-4380-8c6e-faac5ec67a15'...\nResults for job 'ca6b0fbf-061d-4380-8c6e-faac5ec67a15' have been written to...\n./results\nFetching results of job '9d05a2a4-9981-4763-8490-3b849eaa628b'...\nResults for job '9d05a2a4-9981-4763-8490-3b849eaa628b' have been written to...\n./results\nFetching results of job 'c5baf5cc-3259-4acf-ba75-92e1cb2235bb'...\nResults for job 'c5baf5cc-3259-4acf-ba75-92e1cb2235bb' have been written to...\n./results\nFetching results of job '32f97eaa-cad6-49f0-9b20-a2a1276980be'...\nResults for job '32f97eaa-cad6-49f0-9b20-a2a1276980be' have been written to...\n./results\nFetching results of job '69bd1c82-1be7-4b1f-8ab0-d5d2bef19bb1'...\nResults for job '69bd1c82-1be7-4b1f-8ab0-d5d2bef19bb1' have been written to...\n./results\nFetching results of job '31a43a56-fc31-4254-83b2-8fa8f8a6e542'...\nResults for job '31a43a56-fc31-4254-83b2-8fa8f8a6e542' have been written to...\n./results\nFetching results of job 'ecfdf8f8-dbad-4447-a275-af8fceb6abfc'...\nResults for job 'ecfdf8f8-dbad-4447-a275-af8fceb6abfc' have been written to...\n./results\nFetching results of job 'd45365d4-9cf3-4a24-9ae1-c8c8d8d5856f'...\nResults for job 'd45365d4-9cf3-4a24-9ae1-c8c8d8d5856f' have been written to...\n./results\nFetching results of job '08cbab6e-88a4-4a20-bcd3-26e2c8f379a7'...\nResults for job '08cbab6e-88a4-4a20-bcd3-26e2c8f379a7' have been written to...\n./results\nFetching results of job '93bcb94e-19a3-49ba-af18-ea6566515b93'...\nResults for job '93bcb94e-19a3-49ba-af18-ea6566515b93' have been written to...\n./results\nFetching results of job 'ce6a8215-b5d4-4cb3-b4ac-e24ef1f84e1c'...\nResults for job 'ce6a8215-b5d4-4cb3-b4ac-e24ef1f84e1c' have been written to...\n./results\nFetching results of job '354c3482-4f3a-4d56-b908-43bb8708450f'...\nResults for job '354c3482-4f3a-4d56-b908-43bb8708450f' have been written to...\n./results\nFetching results of job '7cc6e343-2b97-4826-9b73-b5bd32019ffc'...\nResults for job '7cc6e343-2b97-4826-9b73-b5bd32019ffc' have been written to...\n./results\nFetching results of job '3d396eed-abb4-47bc-a4dc-24647c619db9'...\nResults for job '3d396eed-abb4-47bc-a4dc-24647c619db9' have been written to...\n./results\nFetching results of job 'bc2c6361-39dc-4970-8284-be1f7f011c66'...\nResults for job 'bc2c6361-39dc-4970-8284-be1f7f011c66' have been written to...\n./results\nFetching results of job 'f421a3e8-009e-4205-898b-9811a78d6a36'...\nResults for job 'f421a3e8-009e-4205-898b-9811a78d6a36' have been written to...\n./results\nFetching results of job 'ab73b414-edac-45f9-95e6-06a855629b23'...\nResults for job 'ab73b414-edac-45f9-95e6-06a855629b23' have been written to...\n./results\nFetching results of job '44711531-3aff-4b65-9db9-9fc6d3948023'...\nResults for job '44711531-3aff-4b65-9db9-9fc6d3948023' have been written to...\n./results\nFetching results of job 'f6143867-f2ab-4701-a85a-375f080bcd3d'...\nResults for job 'f6143867-f2ab-4701-a85a-375f080bcd3d' have been written to...\n./results\nFetching results of job '6b8552f8-8505-4139-bb88-ab5e5e0b8cef'...\nResults for job '6b8552f8-8505-4139-bb88-ab5e5e0b8cef' have been written to...\n./results\nFetching results of job 'd3e25508-7b28-4e83-be0d-be921dbfd7e9'...\nError: error downloading job: failed to write to '/Users/phil/source/bacalhau-project/examples/data-engineering/blockchain-etl/results/raw/QmdKAxhWU527asddidm49w3CuX4hymR4RSrXGuuHpVAyiC': context deadline exceeded\nUsage:\n  bacalhau get [id] [flags]\n\nExamples:\n  # Get the results of a job.\n  bacalhau get 51225160-807e-48b8-88c9-28311c7899e1\n  \n  # Get the results of a job, with a short ID.\n  bacalhau get ebd9bf2f\n\nFlags:\n      --download-timeout-secs int   Timeout duration for IPFS downloads. (default 300)\n  -h, --help                        help for get\n      --ipfs-swarm-addrs string     Comma-separated list of IPFS nodes to connect to.\n      --output-dir string           Directory to write the output to.\n\nGlobal Flags:\n      --api-host string   The host for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_HOST environment variable is set. (default \"bootstrap.production.bacalhau.org\")\n      --api-port int      The port for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_PORT environment variable is set. (default 1234)\n\nerror downloading job: failed to write to '/Users/phil/source/bacalhau-project/examples/data-engineering/blockchain-etl/results/raw/QmdKAxhWU527asddidm49w3CuX4hymR4RSrXGuuHpVAyiC': context deadline exceeded\nFetching results of job '0f4dfdb6-bf13-41af-bc7a-e045ac360ba7'...\nError: error downloading job: failed to get ipfs cid 'QmW6bdwq3xWp8dJmJ6SRdQK3wG8jqzEVUqh6zCog2ZzgHC': Post \"http://127.0.0.1:54194/api/v0/files/stat?arg=%2Fipfs%2FQmW6bdwq3xWp8dJmJ6SRdQK3wG8jqzEVUqh6zCog2ZzgHC\": context deadline exceeded\nUsage:\n  bacalhau get [id] [flags]\n\nExamples:\n  # Get the results of a job.\n  bacalhau get 51225160-807e-48b8-88c9-28311c7899e1\n  \n  # Get the results of a job, with a short ID.\n  bacalhau get ebd9bf2f\n\nFlags:\n      --download-timeout-secs int   Timeout duration for IPFS downloads. (default 300)\n  -h, --help                        help for get\n      --ipfs-swarm-addrs string     Comma-separated list of IPFS nodes to connect to.\n      --output-dir string           Directory to write the output to.\n\nGlobal Flags:\n      --api-host string   The host for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_HOST environment variable is set. (default \"bootstrap.production.bacalhau.org\")\n      --api-port int      The port for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_PORT environment variable is set. (default 1234)\n\nerror downloading job: failed to get ipfs cid 'QmW6bdwq3xWp8dJmJ6SRdQK3wG8jqzEVUqh6zCog2ZzgHC': Post \"http://127.0.0.1:54194/api/v0/files/stat?arg=%2Fipfs%2FQmW6bdwq3xWp8dJmJ6SRdQK3wG8jqzEVUqh6zCog2ZzgHC\": context deadline exceeded\nFetching results of job 'cc16970e-a5be-42f4-bec5-098acbe4d7d9'...\nError: error downloading job: failed to get ipfs cid 'QmSVm6z4Z9MD5thPjo4yPKKYcerNTnLbyjniSvzD1PNr8m': Post \"http://127.0.0.1:62376/api/v0/files/stat?arg=%2Fipfs%2FQmSVm6z4Z9MD5thPjo4yPKKYcerNTnLbyjniSvzD1PNr8m\": context deadline exceeded\nUsage:\n  bacalhau get [id] [flags]\n\nExamples:\n  # Get the results of a job.\n  bacalhau get 51225160-807e-48b8-88c9-28311c7899e1\n  \n  # Get the results of a job, with a short ID.\n  bacalhau get ebd9bf2f\n\nFlags:\n      --download-timeout-secs int   Timeout duration for IPFS downloads. (default 300)\n  -h, --help                        help for get\n      --ipfs-swarm-addrs string     Comma-separated list of IPFS nodes to connect to.\n      --output-dir string           Directory to write the output to.\n\nGlobal Flags:\n      --api-host string   The host for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_HOST environment variable is set. (default \"bootstrap.production.bacalhau.org\")\n      --api-port int      The port for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_PORT environment variable is set. (default 1234)\n\nerror downloading job: failed to get ipfs cid 'QmSVm6z4Z9MD5thPjo4yPKKYcerNTnLbyjniSvzD1PNr8m': Post \"http://127.0.0.1:62376/api/v0/files/stat?arg=%2Fipfs%2FQmSVm6z4Z9MD5thPjo4yPKKYcerNTnLbyjniSvzD1PNr8m\": context deadline exceeded\nFetching results of job '37819801-f09b-4d3c-86ce-070a4f8c37a3'...\nError: error downloading job: failed to get ipfs cid 'QmUG36RPRZXdop76dXcgiyW2mxeYkARapRmeAh6EcTHP81': Post \"http://127.0.0.1:51957/api/v0/files/stat?arg=%2Fipfs%2FQmUG36RPRZXdop76dXcgiyW2mxeYkARapRmeAh6EcTHP81\": context deadline exceeded\nUsage:\n  bacalhau get [id] [flags]\n\nExamples:\n  # Get the results of a job.\n  bacalhau get 51225160-807e-48b8-88c9-28311c7899e1\n  \n  # Get the results of a job, with a short ID.\n  bacalhau get ebd9bf2f\n\nFlags:\n      --download-timeout-secs int   Timeout duration for IPFS downloads. (default 300)\n  -h, --help                        help for get\n      --ipfs-swarm-addrs string     Comma-separated list of IPFS nodes to connect to.\n      --output-dir string           Directory to write the output to.\n\nGlobal Flags:\n      --api-host string   The host for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_HOST environment variable is set. (default \"bootstrap.production.bacalhau.org\")\n      --api-port int      The port for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_PORT environment variable is set. (default 1234)\n\nerror downloading job: failed to get ipfs cid 'QmUG36RPRZXdop76dXcgiyW2mxeYkARapRmeAh6EcTHP81': Post \"http://127.0.0.1:51957/api/v0/files/stat?arg=%2Fipfs%2FQmUG36RPRZXdop76dXcgiyW2mxeYkARapRmeAh6EcTHP81\": context deadline exceeded\nFetching results of job 'd55cc9e7-9e33-4485-9137-2b5c7e1e831a'...\nResults for job 'd55cc9e7-9e33-4485-9137-2b5c7e1e831a' have been written to...\n./results\nFetching results of job '8313e946-e587-468a-83e1-e464138d3935'...\nResults for job '8313e946-e587-468a-83e1-e464138d3935' have been written to...\n./results\nFetching results of job '65681fa0-44ee-46a4-ab46-6db705d1d318'...\nResults for job '65681fa0-44ee-46a4-ab46-6db705d1d318' have been written to...\n./results\nFetching results of job 'c164e617-01a9-4967-ac29-f2c5e2ddeb3f'...\nResults for job 'c164e617-01a9-4967-ac29-f2c5e2ddeb3f' have been written to...\n./results\nFetching results of job '0e0b5e9b-d467-41f6-965e-249f06e7a32b'...\nResults for job '0e0b5e9b-d467-41f6-965e-249f06e7a32b' have been written to...\n./results\nFetching results of job 'eb733f15-a129-499e-a59c-8eaba0dd68cc'...\nResults for job 'eb733f15-a129-499e-a59c-8eaba0dd68cc' have been written to...\n./results\nFetching results of job 'fcf637d3-1817-4a99-9884-9db568e4d18d'...\nResults for job 'fcf637d3-1817-4a99-9884-9db568e4d18d' have been written to...\n./results\nFetching results of job 'e9450225-c2fb-47ea-b735-ebbde28bdd33'...\nError: error downloading job: failed to get ipfs cid 'QmSraHbxuprHTk92AXppyokhRmcbUwUziDJYvueGva8SQr': Post \"http://127.0.0.1:51682/api/v0/files/stat?arg=%2Fipfs%2FQmSraHbxuprHTk92AXppyokhRmcbUwUziDJYvueGva8SQr\": context deadline exceeded\nUsage:\n  bacalhau get [id] [flags]\n\nExamples:\n  # Get the results of a job.\n  bacalhau get 51225160-807e-48b8-88c9-28311c7899e1\n  \n  # Get the results of a job, with a short ID.\n  bacalhau get ebd9bf2f\n\nFlags:\n      --download-timeout-secs int   Timeout duration for IPFS downloads. (default 300)\n  -h, --help                        help for get\n      --ipfs-swarm-addrs string     Comma-separated list of IPFS nodes to connect to.\n      --output-dir string           Directory to write the output to.\n\nGlobal Flags:\n      --api-host string   The host for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_HOST environment variable is set. (default \"bootstrap.production.bacalhau.org\")\n      --api-port int      The port for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_PORT environment variable is set. (default 1234)\n\nerror downloading job: failed to get ipfs cid 'QmSraHbxuprHTk92AXppyokhRmcbUwUziDJYvueGva8SQr': Post \"http://127.0.0.1:51682/api/v0/files/stat?arg=%2Fipfs%2FQmSraHbxuprHTk92AXppyokhRmcbUwUziDJYvueGva8SQr\": context deadline exceeded\nFetching results of job '2183006a-3992-4c7f-9200-c4c4e0cd69ab'...\nError: error downloading job: failed to get ipfs cid 'QmR81jwpXVFJ6tLtgyokdfcVCJpZGfLjqjHvzo9ZG5UuWg': Post \"http://127.0.0.1:60329/api/v0/files/stat?arg=%2Fipfs%2FQmR81jwpXVFJ6tLtgyokdfcVCJpZGfLjqjHvzo9ZG5UuWg\": context deadline exceeded\nUsage:\n  bacalhau get [id] [flags]\n\nExamples:\n  # Get the results of a job.\n  bacalhau get 51225160-807e-48b8-88c9-28311c7899e1\n  \n  # Get the results of a job, with a short ID.\n  bacalhau get ebd9bf2f\n\nFlags:\n      --download-timeout-secs int   Timeout duration for IPFS downloads. (default 300)\n  -h, --help                        help for get\n      --ipfs-swarm-addrs string     Comma-separated list of IPFS nodes to connect to.\n      --output-dir string           Directory to write the output to.\n\nGlobal Flags:\n      --api-host string   The host for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_HOST environment variable is set. (default \"bootstrap.production.bacalhau.org\")\n      --api-port int      The port for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_PORT environment variable is set. (default 1234)\n\nerror downloading job: failed to get ipfs cid 'QmR81jwpXVFJ6tLtgyokdfcVCJpZGfLjqjHvzo9ZG5UuWg': Post \"http://127.0.0.1:60329/api/v0/files/stat?arg=%2Fipfs%2FQmR81jwpXVFJ6tLtgyokdfcVCJpZGfLjqjHvzo9ZG5UuWg\": context deadline exceeded\nFetching results of job 'f86f2507-d408-4b70-8cdb-47f3ff2083a2'...\nResults for job 'f86f2507-d408-4b70-8cdb-47f3ff2083a2' have been written to...\n./results\nFetching results of job 'cb0670d4-eaa5-4181-808d-3bc92da9b4b8'...\nResults for job 'cb0670d4-eaa5-4181-808d-3bc92da9b4b8' have been written to...\n./results\nFetching results of job '868707f5-dfdc-48e3-ab44-f4d450922a0d'...\nError: error downloading job: failed to get ipfs cid 'QmRUjZFGXqAiEHVKNCbqpK5j6NE9MP8Jr3M35UuBiMtYZ5': Post \"http://127.0.0.1:51117/api/v0/files/stat?arg=%2Fipfs%2FQmRUjZFGXqAiEHVKNCbqpK5j6NE9MP8Jr3M35UuBiMtYZ5\": context deadline exceeded\nUsage:\n  bacalhau get [id] [flags]\n\nExamples:\n  # Get the results of a job.\n  bacalhau get 51225160-807e-48b8-88c9-28311c7899e1\n  \n  # Get the results of a job, with a short ID.\n  bacalhau get ebd9bf2f\n\nFlags:\n      --download-timeout-secs int   Timeout duration for IPFS downloads. (default 300)\n  -h, --help                        help for get\n      --ipfs-swarm-addrs string     Comma-separated list of IPFS nodes to connect to.\n      --output-dir string           Directory to write the output to.\n\nGlobal Flags:\n      --api-host string   The host for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_HOST environment variable is set. (default \"bootstrap.production.bacalhau.org\")\n      --api-port int      The port for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_PORT environment variable is set. (default 1234)\n\nerror downloading job: failed to get ipfs cid 'QmRUjZFGXqAiEHVKNCbqpK5j6NE9MP8Jr3M35UuBiMtYZ5': Post \"http://127.0.0.1:51117/api/v0/files/stat?arg=%2Fipfs%2FQmRUjZFGXqAiEHVKNCbqpK5j6NE9MP8Jr3M35UuBiMtYZ5\": context deadline exceeded\nFetching results of job '2dfc0ce6-d685-424b-95ff-921ad5da0812'...\nResults for job '2dfc0ce6-d685-424b-95ff-921ad5da0812' have been written to...\n./results\nFetching results of job 'd034fd07-e7fb-41c7-b8d3-5c9b1ea01fad'...\nError: error downloading job: failed to get ipfs cid 'QmRYZiQrdPZoXfdCmHXqSzn4X9pCuhWhSTbELR6UBu9wt8': Post \"http://127.0.0.1:56199/api/v0/files/stat?arg=%2Fipfs%2FQmRYZiQrdPZoXfdCmHXqSzn4X9pCuhWhSTbELR6UBu9wt8\": context deadline exceeded\nUsage:\n  bacalhau get [id] [flags]\n\nExamples:\n  # Get the results of a job.\n  bacalhau get 51225160-807e-48b8-88c9-28311c7899e1\n  \n  # Get the results of a job, with a short ID.\n  bacalhau get ebd9bf2f\n\nFlags:\n      --download-timeout-secs int   Timeout duration for IPFS downloads. (default 300)\n  -h, --help                        help for get\n      --ipfs-swarm-addrs string     Comma-separated list of IPFS nodes to connect to.\n      --output-dir string           Directory to write the output to.\n\nGlobal Flags:\n      --api-host string   The host for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_HOST environment variable is set. (default \"bootstrap.production.bacalhau.org\")\n      --api-port int      The port for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_PORT environment variable is set. (default 1234)\n\nerror downloading job: failed to get ipfs cid 'QmRYZiQrdPZoXfdCmHXqSzn4X9pCuhWhSTbELR6UBu9wt8': Post \"http://127.0.0.1:56199/api/v0/files/stat?arg=%2Fipfs%2FQmRYZiQrdPZoXfdCmHXqSzn4X9pCuhWhSTbELR6UBu9wt8\": context deadline exceeded\nFetching results of job '3e40318b-ba7a-4d67-9dfb-217e4776c62a'...\nResults for job '3e40318b-ba7a-4d67-9dfb-217e4776c62a' have been written to...\n./results\nFetching results of job 'a2e17933-5eb5-4866-bb01-a88fb9429608'...\nResults for job 'a2e17933-5eb5-4866-bb01-a88fb9429608' have been written to...\n./results\nFetching results of job 'd1407c23-a485-43a3-87cf-9aeb28e656bf'...\nResults for job 'd1407c23-a485-43a3-87cf-9aeb28e656bf' have been written to...\n./results\nFetching results of job '55ef4899-03af-4667-8c93-d00c01d8d657'...\nResults for job '55ef4899-03af-4667-8c93-d00c01d8d657' have been written to...\n./results\nFetching results of job '7085672f-0571-4a41-b5f7-a983fda9e2b1'...\nResults for job '7085672f-0571-4a41-b5f7-a983fda9e2b1' have been written to...\n./results\nFetching results of job '1ba46518-bf45-45e5-afa6-928844f98e5f'...\nError: error downloading job: failed to get ipfs cid 'QmWxxBym5RssaiTajR4qzLA4tgDpXQMGRutqSkZe6ZGzN8': Post \"http://127.0.0.1:63102/api/v0/files/stat?arg=%2Fipfs%2FQmWxxBym5RssaiTajR4qzLA4tgDpXQMGRutqSkZe6ZGzN8\": context deadline exceeded\nUsage:\n  bacalhau get [id] [flags]\n\nExamples:\n  # Get the results of a job.\n  bacalhau get 51225160-807e-48b8-88c9-28311c7899e1\n  \n  # Get the results of a job, with a short ID.\n  bacalhau get ebd9bf2f\n\nFlags:\n      --download-timeout-secs int   Timeout duration for IPFS downloads. (default 300)\n  -h, --help                        help for get\n      --ipfs-swarm-addrs string     Comma-separated list of IPFS nodes to connect to.\n      --output-dir string           Directory to write the output to.\n\nGlobal Flags:\n      --api-host string   The host for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_HOST environment variable is set. (default \"bootstrap.production.bacalhau.org\")\n      --api-port int      The port for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_PORT environment variable is set. (default 1234)\n\nerror downloading job: failed to get ipfs cid 'QmWxxBym5RssaiTajR4qzLA4tgDpXQMGRutqSkZe6ZGzN8': Post \"http://127.0.0.1:63102/api/v0/files/stat?arg=%2Fipfs%2FQmWxxBym5RssaiTajR4qzLA4tgDpXQMGRutqSkZe6ZGzN8\": context deadline exceeded\nFetching results of job 'ede7c3a2-3cf1-4d08-abe3-278f55d0c27e'...\nError: error downloading job: failed to get ipfs cid 'QmVHcd4HrnhALbJSR8BvPjpkv5RGzaxeitCosa1fwp6Mfy': Post \"http://127.0.0.1:53669/api/v0/files/stat?arg=%2Fipfs%2FQmVHcd4HrnhALbJSR8BvPjpkv5RGzaxeitCosa1fwp6Mfy\": context deadline exceeded\nUsage:\n  bacalhau get [id] [flags]\n\nExamples:\n  # Get the results of a job.\n  bacalhau get 51225160-807e-48b8-88c9-28311c7899e1\n  \n  # Get the results of a job, with a short ID.\n  bacalhau get ebd9bf2f\n\nFlags:\n      --download-timeout-secs int   Timeout duration for IPFS downloads. (default 300)\n  -h, --help                        help for get\n      --ipfs-swarm-addrs string     Comma-separated list of IPFS nodes to connect to.\n      --output-dir string           Directory to write the output to.\n\nGlobal Flags:\n      --api-host string   The host for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_HOST environment variable is set. (default \"bootstrap.production.bacalhau.org\")\n      --api-port int      The port for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_PORT environment variable is set. (default 1234)\n\nerror downloading job: failed to get ipfs cid 'QmVHcd4HrnhALbJSR8BvPjpkv5RGzaxeitCosa1fwp6Mfy': Post \"http://127.0.0.1:53669/api/v0/files/stat?arg=%2Fipfs%2FQmVHcd4HrnhALbJSR8BvPjpkv5RGzaxeitCosa1fwp6Mfy\": context deadline exceeded\nFetching results of job '7aa140ab-79c6-452f-a3f6-19035e7187cd'...\nError: error downloading job: failed to get ipfs cid 'QmQahTAxdLWxCCH77GwwwAvCtLz8PZ4V1zD3aKGD2cbrBM': Post \"http://127.0.0.1:59035/api/v0/files/stat?arg=%2Fipfs%2FQmQahTAxdLWxCCH77GwwwAvCtLz8PZ4V1zD3aKGD2cbrBM\": context deadline exceeded\nUsage:\n  bacalhau get [id] [flags]\n\nExamples:\n  # Get the results of a job.\n  bacalhau get 51225160-807e-48b8-88c9-28311c7899e1\n  \n  # Get the results of a job, with a short ID.\n  bacalhau get ebd9bf2f\n\nFlags:\n      --download-timeout-secs int   Timeout duration for IPFS downloads. (default 300)\n  -h, --help                        help for get\n      --ipfs-swarm-addrs string     Comma-separated list of IPFS nodes to connect to.\n      --output-dir string           Directory to write the output to.\n\nGlobal Flags:\n      --api-host string   The host for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_HOST environment variable is set. (default \"bootstrap.production.bacalhau.org\")\n      --api-port int      The port for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_PORT environment variable is set. (default 1234)\n\nerror downloading job: failed to get ipfs cid 'QmQahTAxdLWxCCH77GwwwAvCtLz8PZ4V1zD3aKGD2cbrBM': Post \"http://127.0.0.1:59035/api/v0/files/stat?arg=%2Fipfs%2FQmQahTAxdLWxCCH77GwwwAvCtLz8PZ4V1zD3aKGD2cbrBM\": context deadline exceeded\nFetching results of job '31787f8f-a566-4b2f-b3f4-91390d3bea68'...\nError: error downloading job: failed to get ipfs cid 'QmXtbqVopAhc7DWZNrnDKEB7zfJ3gfdHytSFGSGaX9HS8i': Post \"http://127.0.0.1:64785/api/v0/files/stat?arg=%2Fipfs%2FQmXtbqVopAhc7DWZNrnDKEB7zfJ3gfdHytSFGSGaX9HS8i\": context deadline exceeded\nUsage:\n  bacalhau get [id] [flags]\n\nExamples:\n  # Get the results of a job.\n  bacalhau get 51225160-807e-48b8-88c9-28311c7899e1\n  \n  # Get the results of a job, with a short ID.\n  bacalhau get ebd9bf2f\n\nFlags:\n      --download-timeout-secs int   Timeout duration for IPFS downloads. (default 300)\n  -h, --help                        help for get\n      --ipfs-swarm-addrs string     Comma-separated list of IPFS nodes to connect to.\n      --output-dir string           Directory to write the output to.\n\nGlobal Flags:\n      --api-host string   The host for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_HOST environment variable is set. (default \"bootstrap.production.bacalhau.org\")\n      --api-port int      The port for the client and server to communicate on (via REST).\n                          Ignored if BACALHAU_API_PORT environment variable is set. (default 1234)\n\nerror downloading job: failed to get ipfs cid 'QmXtbqVopAhc7DWZNrnDKEB7zfJ3gfdHytSFGSGaX9HS8i': Post \"http://127.0.0.1:64785/api/v0/files/stat?arg=%2Fipfs%2FQmXtbqVopAhc7DWZNrnDKEB7zfJ3gfdHytSFGSGaX9HS8i\": context deadline exceeded\nFetching results of job 'b167a9f3-1c5f-40e8-b7c5-e864221deab9'...\nResults for job 'b167a9f3-1c5f-40e8-b7c5-e864221deab9' have been written to...\n./results\nFetching results of job 'adab8249-2fb8-49b4-bc01-faac00810a26'...\nResults for job 'adab8249-2fb8-49b4-bc01-faac00810a26' have been written to...\n./results\n\n\n2022/11/22 09:22:31 CleanupManager.fnsMutex violation CRITICAL section took 20.054ms 20054000 (threshold 10ms)\n2022/11/22 09:25:46 CleanupManager.fnsMutex violation CRITICAL section took 19.095ms 19095000 (threshold 10ms)\n2022/11/22 09:28:03 CleanupManager.fnsMutex violation CRITICAL section took 19.208ms 19208000 (threshold 10ms)\ncp: cannot stat 'results/combined_results/outputs/*': No such file or directory\ncp: cannot stat 'results/combined_results/outputs/*': No such file or directory\ncp: cannot stat 'results/combined_results/outputs/*': No such file or directory\ncp: cannot stat 'results/combined_results/outputs/*': No such file or directory\n2022/11/22 09:48:21 CleanupManager.fnsMutex violation CRITICAL section took 19.451ms 19451000 (threshold 10ms)\n09:49:41.718 | ??? providerquerymanager/providerquerymanager.go:344 > ERROR bitswap Received provider (12D3KooWGE4R98vokeLsRVdTv8D6jhMnifo81mm7NMRV8WJPNVHb) for cid (QmfV7e9iVDbSg6uLQFypUiMc2DH23PKuPBWcwZMmHYGQ2L) not requested\n\n2022/11/22 09:49:42 CleanupManager.fnsMutex violation CRITICAL section took 19.284ms 19284000 (threshold 10ms)\ncp: cannot stat 'results/combined_results/outputs/*': No such file or directory\ncp: cannot stat 'results/combined_results/outputs/*': No such file or directory\n2022/11/22 10:00:56 CleanupManager.fnsMutex violation CRITICAL section took 19.428ms 19428000 (threshold 10ms)\ncp: cannot stat 'results/combined_results/outputs/*': No such file or directory\n2022/11/22 10:06:02 CleanupManager.fnsMutex violation CRITICAL section took 20.031ms 20031000 (threshold 10ms)\ncp: cannot stat 'results/combined_results/outputs/*': No such file or directory\n2022/11/22 10:11:08 CleanupManager.fnsMutex violation CRITICAL section took 19.805ms 19805000 (threshold 10ms)\n2022/11/22 10:11:25 CleanupManager.fnsMutex violation CRITICAL section took 20.092ms 20092000 (threshold 10ms)\ncp: cannot stat 'results/combined_results/outputs/*': No such file or directory\ncp: cannot stat 'results/combined_results/outputs/*': No such file or directory\ncp: cannot stat 'results/combined_results/outputs/*': No such file or directory\ncp: cannot stat 'results/combined_results/outputs/*': No such file or directory\n2022/11/22 10:31:40 CleanupManager.fnsMutex violation CRITICAL section took 19.283ms 19283000 (threshold 10ms)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import os, glob\nimport pandas as pd\n\n# Get CSV files list from a folder\npath = os.path.join("combined_results", "*.csv")\ncsv_files = glob.glob(path)\n\n# Read each CSV file into a list of DataFrames\ndf_list = (pd.read_csv(file, index_col=\'block_datetime\') for file in csv_files)\n\n# Concatenate all DataFrames\ndf_unsorted = pd.concat(df_list, ignore_index=False)\n\n# Some files will cross days, so group by day and sum the values\ndf = df_unsorted.groupby(level=0).sum()\n\n# Plot\ndf.plot(figsize=(16,9))\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"<AxesSubplot:xlabel='block_datetime'>\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"png",src:a(874).Z,width:"930",height:"546"})),(0,o.kt)("p",null,"That's it! There is several years of Ethereum transaction volume data."),(0,o.kt)("h2",{id:"appendix-1-list-ethereum-data-cids"},"Appendix 1: List Ethereum Data CIDs"),(0,o.kt)("p",null,"The following list is a list of IPFS CID's for the Ethereum data that we used in this tutorial. You can use these CID's to download the rest of the chain if you so desire. The CIDs are ordered by block number and they increase 50,000 blocks at a time. Here's a list of ordered CIDs:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"%%writefile hashes.txt\nbafybeihvtzberlxrsz4lvzrzvpbanujmab3hr5okhxtbgv2zvonqos2l3i\nbafybeifb25fgxrzu45lsc47gldttomycqcsao22xa2gtk2ijbsa5muzegq\nbafybeig4wwwhs63ly6wbehwd7tydjjtnw425yvi2tlzt3aii3pfcj6hvoq\nbafybeievpb5q372q3w5fsezflij3wlpx6thdliz5xowimunoqushn3cwka\nbafybeih6te26iwf5kzzby2wqp67m7a5pmwilwzaciii3zipvhy64utikre\nbafybeicjd4545xph6rcyoc74wvzxyaz2vftapap64iqsp5ky6nz3f5yndm\nbafybeicgo3iofo3sw73wenc3nkdhi263yytjnds5cxjwvypwekbz4sk7ra\nbafybeihvep5xsvxm44lngmmeysihsopcuvcr34an4idz45ixl5slsqzy3y\nbafybeigmt2zwzrbzwb4q2kt2ihlv34ntjjwujftvabrftyccwzwdypama4\nbafybeiciwui7sw3zqkvp4d55p4woq4xgjlstrp3mzxl66ab5ih5vmeozci\nbafybeicpmotdsj2ambf666b2jkzp2gvg6tadr6acxqw2tmdlmsruuggbbu\nbafybeigefo3esovbveavllgv5wiheu5w6cnfo72jxe6vmfweco5eq5sfty\nbafybeigvajsumnfwuv7lp7yhr2sr5vrk3bmmuhhnaz53waa2jqv3kgkvsu\nbafybeih2xg2n7ytlunvqxwqlqo5l3daykuykyvhgehoa2arot6dmorstmq\nbafybeihnmq2ltuolnlthb757teihwvvw7wophoag2ihnva43afbeqdtgi4\nbafybeibb34hzu6z2xgo6nhrplt3xntpnucthqlawe3pmzgxccppbxrpudy\nbafybeigny33b4g6gf2hrqzzkfbroprqrimjl5gmb3mnsqu655pbbny6tou\nbafybeifgqjvmzbtz427bne7af5tbndmvniabaex77us6l637gqtb2iwlwq\nbafybeibryqj62l45pxjhdyvgdc44p3suhvt4xdqc5jpx474gpykxwgnw2e\nbafybeidme3fkigdjaifkjfbwn76jk3fcqdogpzebtotce6ygphlujaecla\nbafybeig7myc3eg3h2g5mk2co7ybte4qsuremflrjneer6xk3pghjwmcwbi\nbafybeic3x2r5rrd3fdpdqeqax4bszcciwepvbpjl7xdv6mkwubyqizw5te\nbafybeihxutvxg3bw7fbwohq4gvncrk3hngkisrtkp52cu7qu7tfcuvktnq\nbafybeicumr67jkyarg5lspqi2w4zqopvgii5dgdbe5vtbbq53mbyftduxy\nbafybeiecn2cdvefvdlczhz6i4afbkabf5pe5yqrcsgdvlw5smme2tw7em4\nbafybeiaxh7dhg4krgkil5wqrv5kdsc3oewwy6ym4n3545ipmzqmxaxrqf4\nbafybeiclcqfzinrmo3adr4lg7sf255faioxjfsolcdko3i4x7opx7xrqii\nbafybeicjmeul7c2dxhmaudawum4ziwfgfkvbgthgtliggfut5tsc77dx7q\nbafybeialziupik7csmhfxnhuss5vrw37kmte7rmboqovp4cpq5hj4insda\nbafybeid7ecwdrw7pb3fnkokq5adybum6s5ok3yi2lw4m3edjpuy65zm4ji\nbafybeibuxwnl5ogs4pwa32xriqhch24zbrw44rp22hrly4t6roh6rz7j4m\nbafybeicxvy47jpvv3fi5umjatem5pxabfrbkzxiho7efu6mpidjpatte54\nbafybeifynb4mpqrbsjbeqtxpbuf6y4frrtjrc4tm7cnmmui7gbjkckszrq\nbafybeidcgnbhguyfaahkoqbyy2z525d3qfzdtbjuk4e75wkdbnkcafvjei\nbafybeiefc67s6hpydnsqdgypbunroqwkij5j26sfmc7are7yxvg45uuh7i\nbafybeiefwjy3o42ovkssnm7iihbog46k5grk3gobvvkzrqvof7p6xbgowi\nbafybeihpydd3ivtza2ql5clatm5fy7ocych7t4czu46sbc6c2ykrbwk5uu\nbafybeiet7222lqfmzogur3zlxqavlnd3lt3qryw5yi5rhuiqeqg4w7c3qu\nbafybeihwomd4ygoydvj5kh24wfwk5kszmst5vz44zkl6yibjargttv7sly\nbafybeidbjt2ckr4oooio3jsfk76r3bsaza5trjvt7u36slhha5ksoc5gv4\nbafybeifyjrmopgtfmswq7b4pfscni46doy3g3z6vi5rrgpozc6duebpmuy\nbafybeidsrowz46yt62zs64q2mhirlc3rsmctmi3tluorsts53vppdqjj7e\nbafybeiggntql57bw24bw6hkp2yqd3qlyp5oxowo6q26wsshxopfdnzsxhq\nbafybeidguz36u6wakx4e5ewuhslsfsjmk5eff5q7un2vpkrcu7cg5aaqf4\nbafybeiaypwu2b45iunbqnfk2g7bku3nfqveuqp4vlmmwj7o7liyys42uai\nbafybeicaahv7xvia7xojgiecljo2ddrvryzh2af7rb3qqbg5a257da5p2y\nbafybeibgeiijr74rcliwal3e7tujybigzqr6jmtchqrcjdo75trm2ptb4e\nbafybeiba3nrd43ylnedipuq2uoowd4blghpw2z7r4agondfinladcsxlku\nbafybeif3semzitjbxg5lzwmnjmlsrvc7y5htekwqtnhmfi4wxywtj5lgoe\nbafybeiedmsig5uj7rgarsjans2ad5kcb4w4g5iurbryqn62jy5qap4qq2a\nbafybeidyz34bcd3k6nxl7jbjjgceg5eu3szbrbgusnyn7vfl7facpecsce\nbafybeigmq5gch72q3qpk4nipssh7g7msk6jpzns2d6xmpusahkt2lu5m4y\nbafybeicjzoypdmmdt6k54wzotr5xhpzwbgd3c4oqg6mj4qukgvxvdrvzye\nbafybeien55egngdpfvrsxr2jmkewdyha72ju7qaaeiydz2f5rny7drgzta\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Overwriting hashes.txt\n")),(0,o.kt)("h2",{id:"appendix-2-setting-up-an-ethereum-node"},"Appendix 2: Setting up an Ethereum Node"),(0,o.kt)("p",null,"In the course of writing this example I had to setup an Ethereum node. It was a slow and painful process so I thought I would share the steps I took to make it easier for others."),(0,o.kt)("h3",{id:"geth-setup-and-sync"},"Geth setup and sync"),(0,o.kt)("p",null,"Geth supports Ubuntu by default, so use that when creating a VM. Use Ubuntu 22.04 LTS."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"gcloud compute instances create phil-ethereum-node \\\n    --project=bacalhau-development --zone=europe-west2-c \\\n    --machine-type=c2-standard-4 --tags=geth \\\n    --create-disk=auto-delete=yes,boot=yes,device-name=phil-ethereum-node,image=projects/ubuntu-os-cloud/global/images/ubuntu-2204-jammy-v20221101a,mode=rw,size=50,type=projects/bacalhau-development/zones/europe-west2-c/diskTypes/pd-balanced \\\n    --create-disk=auto-delete=yes,device-name=phil-ethereum-disk,mode=rw,name=phil-ethereum-disk,size=3000,type=projects/bacalhau-development/zones/europe-west2-c/diskTypes/pd-standard\n")),(0,o.kt)("p",null,"Mount the disk:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"sudo mkfs.ext4 -m 0 -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/sdb\nsudo mkdir -p /mnt/disks/ethereum\nsudo mount -o discard,defaults /dev/sdb /mnt/disks/ethereum\nsudo chmod a+w /mnt/disks/ethereum\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"sudo add-apt-repository -y ppa:ethereum/ethereum\nsudo apt-get update\nsudo apt-get install -y ethereum\nsudo mkdir /prysm && cd /prysm\nsudo curl https://raw.githubusercontent.com/prysmaticlabs/prysm/master/prysm.sh --output prysm.sh && sudo chmod +x prysm.sh\n")),(0,o.kt)("p",null,"Run as a new user:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"sudo useradd -d /home/ethuser -m --uid 10000 ethuser\nsudo chown -R ethuser /prysm\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'sudo tee "/etc/systemd/system/geth.service" > /dev/null <<\'EOF\'\n[Unit]\nDescription=Geth\n\n[Service]\nType=simple\nUser=ethuser\nRestart=always\nRestartSec=12\nExecStart=/bin/geth --syncmode "full" --datadir /mnt/disks/ethereum\n\n[Install]\nWantedBy=default.target\nEOF\n\nsudo tee "/etc/systemd/system/prysm.service" > /dev/null <<\'EOF\'\n[Unit]\nDescription=Prysm\n\n[Service]\nType=simple\nUser=ethuser\nRestart=always\nRestartSec=12\nExecStart=/prysm/prysm.sh beacon-chain --execution-endpoint=/mnt/disks/ethereum/geth.ipc --suggested-fee-recipient=0x7f68cb1cdE000AF82291A0D0c21E0f88FD7dB440 --checkpoint-sync-url=https://beaconstate.info\n--genesis-beacon-api-url=https://beaconstate.info --accept-terms-of-use --datadir /mnt/disks/ethereum/prysm\n\n[Install]\nWantedBy=default.target\nEOF\n\nsudo systemctl daemon-reload\nsudo systemctl enable prysm.service\nsudo systemctl enable geth.service\nsudo systemctl daemon-reload\nsudo service prysm start \nsudo service geth start \n')),(0,o.kt)("p",null,"Check they are running:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"service prysm status\nservice geth status\n")),(0,o.kt)("p",null,"Watch the logs:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"journalctl -u prysm -f\n")),(0,o.kt)("p",null,"Prysm will need to finish synchronising before geth will start syncronising."),(0,o.kt)("p",null,"In Prysm you will see lots of log messages saying: ",(0,o.kt)("inlineCode",{parentName:"p"},"Synced new block"),", and in Geth you will see: ",(0,o.kt)("inlineCode",{parentName:"p"},"Syncing beacon headers    downloaded=11,920,384 left=4,054,753  eta=2m25.903s"),". This tells you how long it will take to sync the beacons. Once that's done, get will start synchronising the blocks."),(0,o.kt)("p",null,"Bring up the ethereum javascript console with:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"sudo geth --datadir /mnt/disks/ethereum/ attach\n")),(0,o.kt)("p",null,"Once the block sync has started, ",(0,o.kt)("inlineCode",{parentName:"p"},"eth.syncing")," will return values. Before it starts, this value will be ",(0,o.kt)("inlineCode",{parentName:"p"},"false"),"."),(0,o.kt)("p",null,"Note that by default, geth will perform a fast sync, without downloading the full blocks. The ",(0,o.kt)("inlineCode",{parentName:"p"},"syncmode=flull")," flag forces geth to do a full sync. If we didn't do this, then we wouldn't be able to backup the data properly."),(0,o.kt)("h3",{id:"extracting-the-data"},"Extracting the Data"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"# Install pip and ethereum-etl\nsudo apt-get install -y python3-pip\nsudo pip3 install ethereum-etl\ncd\nmkdir ethereum-etl\ncd ethereum-etl\n\n# Export data with one 50000-item batch in a directory. Up to this point we've processed about 3m.\n# The full chain is about 16m blocks\nfor i in $(seq 0 50000 16000000); do sudo ethereumetl export_all --partition-batch-size 50000 --start $i --end $(expr $i + 50000 - 1)  --provider-uri file:///mnt/disks/ethereum/geth.ipc -o output_$i; done\n")),(0,o.kt)("h3",{id:"upload-the-data"},"Upload the data"),(0,o.kt)("p",null,"Tar and compress the directories to make them easier to upload:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"sudo apt-get install -y jq # Install jq to parse the cid\ncd\ncd ethereum-etl\nfor i in $(seq 0 50000 16000000); do tar cfz output_$i.tar.gz output_$i; done\n")),(0,o.kt)("p",null,"Export your Web3.storage JWT API key as an environment variable called ",(0,o.kt)("inlineCode",{parentName:"p"},"TOKEN"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'printf "" > hashes.txt\nfor i in $(seq 0 50000 16000000); do curl -X POST https://api.web3.storage/upload -H "Authorization: Bearer ${TOKEN}" -H \'accept: application/json\' -H \'Content-Type: text/plain\' -H "X-NAME: ethereum-etl-block-$i" --data-binary "@output_$i.tar.gz" >> raw.json; done\n')))}m.isMDXComponent=!0},9663:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/index_25_1-21939b9e4185fb287b27e442b590c44d.png"},874:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/index_34_1-58538ffb740ef0064b704b3c6fbb69a6.png"},6901:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/index_7_1-ced48435932bb22db33d53e6aab436ca.png"}}]);